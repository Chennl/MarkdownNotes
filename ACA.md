# ACP



%WINDIR%\System32\cmd.exe "/K" C:\ProgramData\miniconda3\Scripts\activate.bat C:\ProgramData\miniconda3



[访问控制 RAM 控制台](https://ram.console.aliyun.com/authorize?request={"payloads"%3A[{"missionId"%3A"PAI.RoleForPAIDSWDefault"}%2C{"missionId"%3A"PAI.RoleForPAIDLCDefault"}%2C{"missionId"%3A"MaxCompute.RoleForODPSPAIDefault"}%2C{"missionId"%3A"PAI.RoleForPAIAccessingOSS"}%2C{"missionId"%3A"PAI.RoleForPAIDLCAccessingOSS"}%2C{"missionId"%3A"PAI.RoleForPAIDatasetAccDefault"}%2C{"missionId"%3A"PAILangStudio.RoleForPAILangStudioDefault"}%2C{"missionId"%3A"PAI.RoleForPaiCustomerClusterManagement"}]%2C"ReturnUrl"%3A"https%3A%2F%2Fpai.console.aliyun.com%2F%3FregionId%3Dcn-shanghai%26spm%3Da2c4g.11186623.0.0.51323b46KRVZKj%23%2Fworkspace%2Fwelcome"})

获取资料：
pwd
git clone https://atomgit.com/alibabaclouddocs/aliyun_acp_learning.git

《2025年B站最新最全的阿里云ACP大模型认证全套系统教程》[1、准备学习环境_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1zvfaYLEk1/?p=2&vd_source=265631a940f0c085c9ceec6d0254cd21)

1.Python环境

​	

```bash
#1.1 ANACONDA [Download Anaconda Distribution | Anaconda](https://www.anaconda.com/download)
[chennl@Tomhost ~]$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
[chennl@Tomhost ~]$ bash Miniconda3-latest-Linux-x86_64.sh.sh
Thank you for installing Miniconda3!
(base) [chennl@Tomhost ~]$ conda -V
conda 25.7.0

#To accept these channels' Terms of Service, run the following commands:
    conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main
    conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r

(base) [chennl@Tomhost ~]$ conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main
accepted Terms of Service for https://repo.anaconda.com/pkgs/main
(base) [chennl@Tomhost ~]$ conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r
accepted Terms of Service for https://repo.anaconda.com/pkgs/r

(base) [chennl@Tomhost ~]$ conda create -n learnacp python=3.10.12 -y -q

 conda create -n learnacp python=3.12.10 -y

(base) [chennl@Tomhost ~]$ conda activate learnacp
(learnacp) [chennl@Tomhost ~]$ python --version
Python 3.10.12
(learnacp) [chennl@Tomhost ~]$ pip install ipykernel -i https://mirrors.aliyun.com/pypi/simple
# 安装 Jupyter-Notebook
> pip install jupyter -i https://pypi.tuna.tsinghua.edu.cn/simple/
> jupyter notebook
# 切换到ACP目录
> pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/

#创建一 Jupyter 环境内核
(learnacp) [chennl@Tomhost ~]$ conda install jupyter
(learnacp) [chennl@Tomhost ~]$ pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/
#配置Jupyter远程访问
(learnacp) [chennl@Tomhost ~]$ jupyter notebook --generate-config
#当前目录下 /home/chennl
(learnacp) [chennl@Tomhost ~]$ ~/.jupyter/jupyter_notebook_config.json
c.ServerApp.allow_remote_access = True
c.ServerApp.ip = '*'
#保存，运行
[chennl@Tomhost ~]$ Jupyter Notebook
#后台运行方式
[chennl@Tomhost ~]$ nohup jupyter-notebook --ip=0.0.0.0 --port 8000 --no-browser  &



[chennl@Tomhost ~]$ conda config --add channels [chennl@Tomhost ~]$ https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
[chennl@Tomhost ~]$ conda config --add channels [chennl@Tomhost ~]$ https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
[chennl@Tomhost ~]$ conda config --set show_channel_urls yes


firewall-cmd --zone=public --add-port=8888/tcp --permanent 
firewall-cmd --reload

```

1、开放端口

**firewall-cmd --zone=public --add-port=5672/tcp --permanent**  # 开放5672端口

**firewall-cmd --zone=public --remove-port=5672/tcp --permanent** #关闭5672端口

**firewall-cmd --reload**  # 配置立即生效

 

2、查看防火墙所有开放的端口

**firewall-cmd --zone=public --list-ports**

 

3.、关闭防火墙

如果要开放的端口太多，嫌麻烦，可以关闭防火墙，安全性自行评估

**systemctl stop firewalld.service**

 

4、查看防火墙状态

 **firewall-cmd --state**

 

5、查看监听的端口

**netstat -lnpt**

6、检查端口被哪个进程占用

**netstat -lnpt |grep 5672**

7、查看进程的详细信息

**ps 6832**

8、中止进程

**kill -9 6832**

http://172.26.13.101:8888/tree?token=33d2464cc478329a475952c785974dee3dced288f38c03ef



 conda create -n learnacp python=3.10.12 -y -q



# 创建一个 3.10 版本的 python 环境

conda create -n learnacp python=3.10.12 -y -q
conda activate learnacp

# 创建一 Jupyter 环境内核

pip install ipykernel
python3 -m ipykernel install --user --name learnacp --display-name "py310(learnacp)"

conda activate learnacp
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/

conda install jupyter



   To access the server, open this file in a browser:
        file:///home/chennl/.local/share/jupyter/runtime/jpserver-1420-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/tree?token=734bc2c35574e29c928cd1a3f94f206b611e1456b3dcf671
        http://127.0.0.1:8888/tree?token=734bc2c35574e29c928cd1a3f94f206b611e1456b3dcf671

API Key sk-58ea70320b084ac2ae549ba65c7c929e

client = OpenAI( api_key=os.getenv("DASHSCOPE_API_KEY"),base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")



```python
os.environ["DASHSCOPE_API_KEY"] = "sk-58ea70320b084ac2ae549ba65c7c929e"
```

# 第一章 认识大模型

## 1 大模型：

### 1.1 人工智能  → 大模型

 - 人工智能（AI）是一门使机器模拟人类智能过程的学科。其中具体包括**学习、推理、自我修正、感知和处理语言**等功能。

 - 人工智能涉及**计算机科学、数学、心理学**等众多领域的知识，通过创建能够实现智能行为的算法或软件系统，来表现出与人类的智能行为相似的特性

 - 技术实现分类：

   - 人工智能(Artificial Inteligence,Al)：

     旨在**研究、设计、构建**具备智能、学习、推理和行动能力的**计算机和机器**。

   - 机器学习(Machine Learning，ML)

     机器学习是一门研究计算机如何在**没有明确编程的情况下**，通过**对数据**进行分析、学习，自动改进其行为或做出预测的学科。它旨在使计算机系统**具备从经验中学习的能力**，以适应新情况、解决问题或完成特定任务。
   
     - 监督学习、
     - 无监督学习
     - 强化学习
   
   - 深度学习(Deep Learning，DL)
   
     - 深度学习是**机器学习的一个分支**
     - 主要使用**神经网络模型**(由多个隐藏层组成)对数据进行学习和表示。
     - 深度学习算法试图模拟人类大脑的工作方式，其灵感来源于神经生物学，它通过对大量数据的学
       习，自动提取出数据的高层次特征和模式，从而实现**图像识别、语音识别、自然语言处理**等任务。
     - 按照架构的不同，神经网络可以分为:**卷积神经网络(CNNS)、循环神经网络(RNNs)、Transformer**
       **网络**等等。
   
   - 生成式人工智能(Generative Al)
   
     - **生成式人工智能**又是**深度学习**中快速增长的子集，23 年热门模型和应用 ChatGPT、Stable
       Diffusion 等都属于生成式人工智能领域，它们使用了大模型提供支持，在**大量原始、未标记的数**
       **据**基础上对深度学习模型进行预训练，使得机器能够“理解”语言甚至图像，并能够根据需要自动
       生成内容。
   
      

### 1.2 大模型

2021年，斯坦福大学，提出了**Foundational Models**（**基础模型**，即**大模型**）的概念。

简单来说，它是一类具有**大量参数（通常在十亿以上）**，能在极为广泛的数据上进行训练，并适用于多种任务和应用的预训练**深度学习模型**。

 - 2022年11月，OpenAI公司发布了ChatGPT：

   一种先进的人工智能**语言模型**，专为**对话交互而设计**，具有强大的**自然语言理解和生成能力**，可以完成撰写论文、邮件、脚本、文案、翻译、代码等任务。

   ChatGPT的发布标志着AI大模型在语言理解与生成能力上的重大突破，对全球AI产业产生了深远影响，开启了人工智能大模型应用的新篇章。

	- 2023年8月，阿里巴巴集团发布了通义千问系列开源大模型，并相继推出了**7B（约70亿参数**）、**72B（约720亿参数**）等不同参数规模的大语言模型版本。目前，通义千问系列大语言模型已升级至Qwen3版本，具备**自然语言理解、文本生成、视觉理解、音频理解、工具使用、角色扮演、作为AI Agent进行互动**等多种能力。

### 1.4 大模型的训练

大模型的训练整体上分为三个阶段：**预训练**、**SFT（监督微调）**，以及***RLHF（基于人类反馈的强化学习）**。

- **预训练（Pre-training）**：

  预训练的过程类似于从婴儿成长为中学生的阶段，在这个阶段我们会学习各种各样的知识，我们的语言习惯、知识体系等重要部分都会形成；**对于大模型来说，在这个阶段它会学习各种不同种类的语料，学习到语言的统计规律和一般知识。**但是大模型在这个阶段只是学会了补全句子，却没有学会怎么样去领会人类的意图，假设我们向预训练的模型提问：“埃菲尔铁塔在哪个国家？”模型有可能不会回答“法国”，而是根据它看到过的语料进行输出：“东方明珠在哪个城市？”这显然不是一个好的答案，因此我们需要让它能够去遵循人类的指示进行回答，这个步骤就是SFT（监督微调）。

- **监督微调（SFT，Supervised Fine Tuning）**：SFT的过程类似于从中学生成长为大学生的阶段，在这个阶段我们会学习到专业知识，比如金融、法律等领域，我们的头脑会更专注于特定领域。**对于大模型来说，在这个阶段它可以学习各种人类的对话语料，甚至是非常专业的垂直领域知识，在监督微调过程之后，它可以按照人类的意图去回答专业领域的问题**。这时候我们向经过SFT的模型提问：“埃菲尔铁塔在哪个国家？”模型大概率会回答“法国”，而不是去补全后边的句子。这时候的模型已经可以按照人类的意图去完成基本的对话功能了，但是模型的回答有时候可能并不符合人类的偏好，它可能会输出一些涉黄、涉政、涉暴或者种族歧视等言论，这时候我们就需要对模型进行RLHF（基于人类反馈的强化学习）

- **基于人类反馈的强化学习（RLHF，Reinforcement Learning from Human Feedback）**：RLHF的过程类似于从大学生步入职场的阶段，在这个阶段我们会开始进行工作，但是我们的工作可能会受到领导和客户的表扬，也有可能会受到批评，我们会根据反馈调整自己的工作方法，争取在职场获得更多的正面反馈。**对于大模型来说，在这个阶段它会针对同一问题进行多次回答，人类会对这些回答打分，大模型会在此阶段学习到如何输出分数最高的回答，使得回答更符合人类的偏好。**



### 1.5 大模型的特点
​	基础模型（大模型）主要有以下四个特点：

基础模型（大模型）主要有以下四个特点：

- **规模和参数量大**

大模型通过其庞大的规模（**拥有从数亿到数千亿级别的参数数量**）来捕获复杂的数据模式，使得它们能够理解和生成极其丰富的信息。

- **适应性和灵活性强**

模型具有很强的适应性和灵活性，能够通过微调（fine-tune）或少样本学习高效地迁移到各种下游任务，有很强的跨域能力。

- **广泛数据集的预训练**

大模型使用大量多样化的数据进行预训练，以学习广泛的知识表示，能够掌握语言、图像等数据的通用特征。

- **计算资源需求大**

巨大的模型规模带来了高昂的计算和资源需求，包括但不限于**数据存储**、**训练时间**、**能量消耗**和**硬件设施**。

### 1.6 大模型的分类

- **大语言模型（LLM）**：

  这类大模型专注于**自然语言处理（NLP）**，旨在处理语言、文章、对话等自然语言文本。

  它们通常基于**深度学习架构（如Transformer模型）**，经过大规模文本数据集训练而成，能够捕捉语言的复杂性，包括语法、语义、语境以及蕴含的文化和社会知识。

  语言大模型典型应用包括**文本生成、问答系统、文本分类、机器翻译、对话系统**等。示例包括：**GPT系列**（OpenAI）：如GPT-3、GPT-3.5、GPT-4等。**Gemini（Google）**：谷歌推出的大型语言模型，用于提供信息丰富的、有创意的文本输出。**通义千问（阿里云）**：阿里云自主研发的超大规模的语言模型。

- **多模态模型**：

  多模态大模型能够同时处理和理解来自不同感知通道（如**文本、图像、音频、视频**等）的数据，并在这些模态之间建立关联和交互。它们能够整合不同类型的输入信息，进行跨模态推理、生成和理解任务。

  多模态大模型的应用涵盖**视觉问答、图像描述生成、跨模态检索、多媒体内容理解**等领域。<img src="https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/yavdejkfxxkfpzlklqgh.png" alt="img" style="zoom: 33%;" />

## 2 大模型是如何工作的

用户在可以使用**自然语言**与大模型交流，用户的文本就是“**提示词**”。提示词越清晰，模型的回答就越符合预期。
大模型处理提示词的工作流程可以分为两部分，第一部分是**分词化与词表映**射，第二部分**为生成文本**。

### 2.1 分词化（Tokenization）与词表映射
 - **分词化（Tokenization）**

   是自然语言处理（NLP）中的重要概念，它是将段落和句子分割成更小的分词（token）的过程。

   分词化有不同的粒度分类：
   **‒ 词粒度**（Word-Level Tokenization）分词化，如上文中例子所示，适用于大多数**西方语言，如英语**。
   **‒ 字符粒度**（Character-Level）分词化是中文最直接的分词方法，它是以**单个汉字为单位进行分词化**。
   ‒ **子词粒度**（Subword-Level）分词化，它将单词分解成更小的单位，比如**词根、词缀**等。这种方法对于处理新词（比如专有名词、网络用语等）特别有效，因为即使是新词，它的组成部分（子词）很可能已经存在于词表中了。

- **词表映射**

每一个token都会通过预先设置好的词表，映射为一个 token id，这是token 的“身份证”，一句话最终会被表示为一个元素为token id的列表，供计算机进行下一步处理。

### 2.2 大语言模型生成文本的过程

大语言模型的工作概括来说是**根据给定的文本预测下一个token**。对我们来说，看似像在对大模型提问，但实际上是**给了大模型一串提示文本，让它可以对后续的文本进行推理**。

- 大模型的推理过程不是一步到位的，当大模型进行推理时，它会基于现有的token，根据概率最大原则预测出下一个最有可能的token，然后将该预测的token加入到输入序列中，并将更新后的输入序列继续输入大模型预测下一个token，这个过程叫做**自回归**。
- 直到**输出特殊token**（如<EOS>，end of sentence，专门用来控制推理何时结束）或**输出长度达到阈值**。

![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/ubfgivcuiajvvmtphfzk.png)

如下面的例子：
用户提问：阿里云成立于什么时间？
模型输出：

![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/viunkyosmrzsfhgbjxot.png)

## 3. 大模型的应用

### 3.1 阿里云的大模型探索

- **通义千问**是阿里云自主研发的超大规模的**语言模型**，在复杂指令理解、文学创作、通用数学、代码理解生成、知识记忆等能力上均达到领先水平。它支持多种语言，还能处理多种分辨率和规格的图像，实现多语言多模态理解。
- **通义万相**是阿里云自主研发**多模态**图像和视频生成**模型**，可提供AI艺术创作，可支持文生图、图生图、图生视频、虚拟模特、个人写真等多场景的图片和视频创作能力。
- 通义千问和通义万相是阿里巴巴通义系列产品中的基础模型。基于它们的能力，结合实际应用场景，阿里云构建了多个**应用模型**
- **百炼大模型服务平台**是基于阿里云通义大模型构建的，面向企业开发者、个人开发者及ISV合作伙伴提供通义系列大模型、三方大模型等调用、模型训练开发及大模型应用构建的服务平台。

### 3.2 通义大模型介绍

- **3.2.1 通义千问**  通义千问是阿里巴巴超大规模语言模型，能帮你写文案、代码，解答问题，提升工作效率，满足个性化创作需求，甚至还能与你进行趣味互动。
- **3.2.2 通义万相**  通义万相是阿里巴巴通义系列多模态模型，支持文生图、文生视频、图生视频等功能，可以用它进行人像风格重绘、视频风格定制等图像类工作。

### 3.3 通义模型应用介绍

- **3.3.1通义灵码**

- **3.3.2 通义听悟**
   这是一款基于通义千问大模型的智能语音转文字和内容分析工具，可以实现会议记录、采访录音整理、学习笔记生成、视频字幕添加等功能。
- **3.3.3 通义法睿**
  如果你有法律方面的需求，无论是**合同审核、法律咨询**还是文书起草等，都可以使用通义法睿。这款产品融合了先进的AI技术和丰富的法律知识，能解答法律问题、审查合同、定位法律信息、撰写法律文书、分析案情，为用户提供专业、高效、便捷的法律服务。

- **3.3.4 通义晓蜜**
  如果你的企业有庞大的**客户咨询需求**，可以考虑使用通义晓蜜来节省人力成本。它是一款基于大模型技术的智能对话解决方案，可以帮助企业实现客户服务的智能化升级。它不仅能够提供7*24小时不间断的服务支持，还具备强大的多轮对话、文档问答等能力，适用于多种业务场景。

# 第二章 用好大模型

## 1 提示词

**提示词（Prompt）： 是用户发送给大语言模型的问题、指令或请求**，来明确地告诉模型用户想要解决的问题或完成的任务，是大语言模型理解用户需求并据此生成相关、准确回答或内容的基础。

对于**大语言模型**来说，提示词就是**用户输入给大语言模型的文本信息**。

<img src="https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/fhcfdugefelwcgfsqhsy.png" alt="img" style="zoom:20%;" />

### 2 模型的典型应用场景

通过巧妙地构造提示词，我们可以在很多工作场景中使用大模型，提升我们的工作效率，主要分为以下四类场景：文本生成、文本编辑、推理分析、总结摘要。接下来，我们将通过案例和动手实验来向大家展示这些应用场景。

## 课时3：提示词优化

【[阿里云培训中心](https://edu.aliyun.com/course/3126500/lesson/343300189)】

### 1.提示词工程

- **提示词工程（Prompt Engineering）**： 

  就是研究如何构建和调整提示词，从而让大语言模型实现各种符合用户预期的任务的过程。就像跟AI沟通的艺术，为了让像Qwen这样的大语言模型更好地理解你的需求，你需要清晰地描述你的需求，提供必要的背景信息，明确告诉AI你想让它做什么。就像跟人沟通一样，你需要不断调整你的表达方式，直到AI理解你的意思，并给出你想要的答案。

- **提示词工程包括以下关键步骤**：

  ![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/hyjadohhyhltllnbmgks.png)

本课程向您介绍构建提示词的各种技巧。纸上得来终觉浅，绝知此事要躬行。学习这一章节时，你可以访问通义千问大语言模型，边学习边实践。

### 2.提示词技巧

为了引导大模型按照我们的要求来完成各项任务，作为使用者，我们需要**不断调整提示词**，**构建有效的提示词**，从而**不断地提升大模型的表现**。

#### **2.1 直接提问**

- **直接提问**，也称为**零样本提示（Zero-Shot Prompting）**，即不给大语言模型提供案例，完全依靠 LLM 理解和处理能力完成任务。

  前文给的提示词例子，都属于直接提问。目前大部分开源和商用的大语言模型已经经过了大量的预训练和指令调试，能够很好的理解并响应用户的直接提问。适用于目标明确、问题简单、答案确定且唯一等场景。
  **直接提问时，可遵循以下原则**：
  ● **简洁**：尽量用最简短的方式表达问题。过于冗长的问题可能包含多余的信息，导致模型理解错误或答非所问。
  ● **具体**：避免抽象的问题，确保问题是具体的，不含糊。
  ● **详细上下文**：如果问题涉及特定上下文或背景信息，要提供足够的详情以帮助模型理解，即使是直接提问也不例外。
  ● **避免歧义**：如果一个词或短语可能有多重含义，要么明确其含义，要么重新表述以消除歧义。
  ● **逻辑清晰**：问题应逻辑连贯，避免出现逻辑上的混淆或矛盾，这样才能促使模型提供有意义的回答。

#### 2.2 增加示例
在提示词中**提供少量（通常几个或几十个）示例**，也称为**少样本提示（Few-Shot Prompting）**，以帮助模型更好地理解任务要求和期望输出。比如：
● 让 LLM 跟随我们所要求的**规范、格式、概念、文法、语气**进行输出。
● **提供进一步推理的参考**，比如让大模型学会数学运算或按照示例方式进行逻辑推理。

● 翻译时，给LLM一些特殊专有名词， 如公司专有名称，KA=Key Account， SQL = Structure query Language。

但是当需要成千上万的专业术语词条示例辅助翻译的场景，这种方式就不够了，也不够方便，我们可以让大语言模型接入专业的术语库，来增强大语言模型的能力，也即**检索增强生成（Retrieval-Augmented Generation）**，简称RAG。

**示例的质量和数量会直接影响回答的结果**，增加示例时可参考以下技巧：
● **精选代表性样本**：选择具**有代表性的示例**，覆盖任务的各种情况，同时**避免使用可能引起模型混淆的极端或边缘案例**，确保模型能从有限的数据中学习到任务的核心特征。
● **保证示例的多样性**：尽可能覆盖任务的各种角度和情境，确保模型能从有限的数据中学习到任务的核心特征。
● **使用相似的格式和结构**：在所有示例中使用相似的提示格式和结构，使模型能够清晰识别输入与输出的关系。
● **让大语言模型生成示例**：实践时，我们还可以先让 LLM 按照提示生成一些示例，再进行筛选或人工调整，以提高示例质量和针对性。

### 2.3 分配角色
赋予模型一个具体的**角色或身份**，如“作为经验丰富的营养师”，来引导模型在特定角色视角下生成回答。
分配角色适用以下场景：
● **需要专业知识或特定视角的问题解答**。例如，模拟老师、医生、律师等回答相关领域的问题。
● **模拟特定人物或角色的语言风格**。例如，模仿某个著名人物（如历史人物或文学角色）语言风格的文本时。
● **进行角色扮演游戏或创作**。在创意写作或角色扮演游戏中扮演指定的角色，与使用者进行互动。
● **在特定行业内进行决策模拟**。例如，模拟一个管理咨询师来分析商业案例或提供商业建议。

分配角色引导模型生成符合特定情境和对话逻辑的内容，可遵循以下**技巧**：
● 明确**角色身份**与**特性**
    ○ 确定角色的基本属性，如年龄、性别、职业、性格、技能、价值观等。
    ○ 赋予角色相关领域的专业知识或特殊背景，如专家、学者、历史人物、虚构角色等。
● 设定角色**目标与动机**
    ○ 为角色设定对话的**目标**，如寻求信息、说服他人、解决问题、分享观点等。
    ○ 揭示角色的内在**动机**，如个人利益、道德信念、情感需求等，有助于塑造角色的真实性和深度。
● **设定角色语言风格**：
    ○ 根据角色性格、教育水平、文化背景等设定其**语言习惯**、用词选择、句式结构、口头禅等。
    ○ 规定角色在对话中的**情绪状态**，如冷静理智、激动愤怒、悲伤失落、幽默风趣等，影响其表达方式。
● **设定角色规则约束**：规定角色在对话中的行为约束，如不得人身攻击、保持礼貌尊重、遵守讨论主题等。
● **动态调整角色设定**：
    ○ 随着对话深入，适时调整角色设定以适应新的情境和话题，如角色态度转变、关系演变、目标更新等。
    ○ 向模型反馈角色表现，如偏离设定、缺乏个性、对话僵化等，及时修正角色设定并引导模型调整。

### 2.4 限定输出风格/格式
大语言模型非常善于撰写论文、文章等内容，不过如果我们仅仅简单地告诉大语言模型一些宽泛的提示，比如：

为了更好的限定“**风格**”，准确引导模型写出符合需求的内容，下面我们介绍一些推荐的技巧：
1. **明确指出所需的内容类型** 文体，如“论文/散文/诗歌/新闻报道/剧本/日记”等。
2. **用形容词限定风格**，如“严谨客观”、“感性抒情”、“幽默诙谐”、“庄重典雅”等。
3. **列举风格（代表人物/作品）示例**，如“仿照鲁迅先生的笔触描述社会现象”或“以J.K.罗琳的叙述风格撰写一段奇幻冒险故事”。
4. **设定语境**与**情感色彩**：为模型设定故事背景、情感基调或角色视角，影响其语言表达和修辞选择，从而形成特定风格。如“以一名二战老兵的视角，深情回忆战场经历”。
5. **规定语言与句式特点**：要求使用特定词汇、短语、成语、俚语、古语等，或强调长句、短句、排比、反问、比喻等修辞手法的运用，以契合特定风格。

我们还可以进一步对内容的输出格式进行限定：
1. **明确输出长度**：如“撰写一篇关于全球气候变化的新闻报道，标题需简洁明快，概括主题，不超过300字”。

2. **段落结构**：规定正文的段落数量、每段的大致内容与逻辑关系，如“文章分为引言、主体（分三点论述）和结论三部分”。

3. **列表与编号**：要求使用项目符号、数字编号等形式列出要点或步骤，如“通过数字编号列出五种有效的时间管理方法，并简要解释”。

4. **引用与注释**：指示何时使用引号、脚注、尾注、参考文献等格式引用他人观点或资料，如“在论述中适当引用至少两篇相关学术论文，并按照APA格式添加引用和参考文献”。

5. **格式标记**：如对齐方式（左对齐、居中、右对齐）、字体样式（加粗、斜体、下划线）、缩进、行距、页眉页脚等进行详细说明，甚至可以要求大模型按照JSON格式输出。

6. **特殊要求**：针对特定场景，如邮件、信函、通知、海报、简历等，规定相应的格式标准，如“按照商务电子邮件的标准格式撰写邀请函，包括收件人、抄送人、主题、问候语、正文、结束语、签名档等部分”。

   ### 2.5 拆解复杂任务
   把一个复杂的任务，拆解成多个稍微简单的任务，让大语言模型分步来思考问题，称为**思维链（Chain-of-Thought Prompting, CoT）提示**。

   这种方式可让大语言模型像人类一样逐步解释或进行推理，从而极大地提升 LLM 的能力。与前面介绍的标准提示不同，该方法不仅寻求答案，还要求模型解释其得出答案的步骤。

   - 思维链提示中没有给 LLM 提供问题解析示例，所以也可以称为**零样本思维链（Zero-shot CoT）**，相对应地在增加了示例后就变成了**少样本思维链（Few-shot Cot）**。

   - 有时 LLM 通过零样本思维链可能会得到错误的答案，可以通过**增加示例的方式**，即**少样本思维链**，帮助 LLM 理解相关任务并正确执行。

     

### 3.提示词框架

#### 3.1 提示词框架的概念
前面的学习中，我们知道了编写提示词的**基础原则包括**：**简单、明确、详细，也学习了零样本提示、少样本提示、分配角色、提示写作风格、限定输出格式等技巧**，这里我们将重新审视一个提示词是如何构成的，

- **提示词构成**：实际上提示词可以包含以下任意要素：
  - **指令 Instruction**：需要模型去做什么，如回答某个问题、撰写某种类型的文章或按照特定格式进行总结。指令应该简洁、明确，确保 LLM 能够理解任务的目标和要求。
  - **背景信息 Context**：背景信息可以包括任务的背景、目的相关的各类信息，还可以为 LLM 设置角色背景、观点立场等信息，LLM 将在此背景下进行回应或生成文本。
  - **参考样本 Examples**：与解决用户问题相关的示例，比如通过少样本提示的方式帮助 LLM 更好理解如何处理指令。
  - **输入数据 Input Data**：用户输入指令和要求，比如用什么语气，生成多少字的内容。
  - **输出指示 Output Indicator**：指定输出的类型或格式，我们可以给出限定关键词、限制条件或要求的输出格式/方式（如表格），也可以避免无关或不期望的信息出现。

结合上述要素，我们可以根据任务的复杂度来设计提示词，具体有以下几种情况：
1. **纯指令型**：最直接的互动方式，仅通过简明指令向模型提出需求，适合于寻求快速、基本答案的场景。
2. **背景+指令**：在指令基础上融入背景信息，为模型创造一个理解和响应任务的框架，尤其适用于需要考虑特定情境或角色定位的任务。
3. **指令/背景+输出指示**：在基础指令或背景信息之上，加入输出指示，精确指导模型如何组织和呈现答案，以满足特定的格式或风格要求。
4. **综合型提示**：结合指令、背景信息、输入数据与输出指示，形成一个全方位的引导体系。这种复合型提示尤为强大，能够在复杂任务中提高模型输出的针对性与质量，尤其是在模型需要从示例中学习并模仿特定风格或结构时。

### 3.2 提示词框架示例
下面是一个包含了“背景信息”+“指令”+“输出指示”的提示词示例：

```
提示词：
背景知识：“阿里云弹性容器实例 ECI（Elastic Container Instance）是敏捷安全的Serverless容器运行服务。您无需管理底层服务器，也无需关心运行过程中的容量规划，只需要提供打包好的Docker镜像，即可运行容器，并仅为容器实际运行消耗的资源付费。”
请参考如上背景知识回答如下问题：
问题：阿里云弹性容器实例 ECI 是用来运行什么的？
回答：分别使用中文和英文回答
```

我们来拆解下这个示例：
**背景信息**：【背景知识：“阿里云...资源付费”】
**指令**：【请参考如上背景知识回答如下问题】
**输入数据**：【问题：阿里云弹性容器实例 ECI 是用来运行什么的？】
**输出指示**：【回答：分别使用中文和英文回答】
这个例子是为了更好的展示提示词的关键要素，一般来说，我们在熟练掌握提示词的使用技巧后，**不会如此显性机械的使用这种格式**，比如我们可以改写如上的例子并向大语言模型提问：

```
阿里云弹性容器实例 ECl(Elastic Container Instance)是敏捷安全的Serverless容器运行服务。您
无需管理底层服务器，也无需关心运行过程中的容量规划，只需要提供打包好的Docker镜像，即
可运行容器，并仅为容器实际运行消耗的资源付费。参考如上信息用中英文双语回答阿里云弹性
容器实例 ECI是用来运行什么的。
```

如果你希望让大模型完成相对复杂的任务可以通过以下方式进行处理：
1. 将处理问题所需要的各种信息抽象为“提示词要素”
2. 将处理问题的方法变为提示词模版
3. 再通过程序批量调用大模型，实现批量生成

```
提示词示例：
## 任务目标：[如“分析电影主题”] 
## 任务背景：[如受众、场景、]
## 你的角色：[如影评人] 
## 输出要求：[如“字数1500”] 
## 分析方法：[“心理学理论应用”] 
## 写作逻辑：
   [1.识别核心冲突]
   [2. 关联学术理论与剧情]
   [3. 总结导演意图] 
## 参考案例：[示例或其他参考资料]
```

### 3.3 常见提示词框架及场景
以上章节讲解了提示词框架，掌握提示词框架，可以用更精准、高效的指令来使用大模型。以下有几种常见的提示词框架，适用于不同的业务领域，供大家参考，你可以根据自己的业务需要来设计提示词。

![企业微信截图_17529949914737](C:\Users\chennl\AppData\Local\Temp\企业微信截图_17529949914737.png)



# 4 推理模型

前面面所讲的提示词技巧和提示词框架可以广泛适用于**通用大模型**(如Qwen2.5-max、GPT-4、DeepSeek-V3)，这类模型面向**通用对话、知识问答、文本生成**等广泛的场景。

###  大模型：

- 通用大模型，

- 推理模型： 专门为“推理”设计的模型

### 4.1 什么是推理模型?
- **推理模型**通常指专门优化用于**逻辑推理、问题解决、多步推断**等任务的模型(如DeepSeek-R1、o1)，
- 它们通常在数学解题, 代码生成、逻辑分析等方面有更好的表现。
- 与通用模型相比，推理模型拥有更强的**逻辑能力**，因为它经过了大量的专项训练，能够更好地理解复杂的问题，并目在解答时更有条理。
- 但并不是说就一定比通用模型更好，两种模型都有各自的应用场景，下表从一些典型维度对这两类模型进行了对比:![企业微信截图_17529956447472](C:\Users\chennl\AppData\Local\Temp\企业微信截图_17529956447472.png)

- 推理模型还是通用模型?如何选择?以下是一些推荐:
  - **明确的通用任务**:对于明确定义的问题，**通用模型**一般能够很好地处理
  - **复杂任务**:对于非常复杂的任务，且需要给出相对**更精确和可靠的答案**，推荐使用**推理模型**。这些任务可能有:
    - 模糊的任务: 任务相关信息很少，你无法提供模型相对明确的指引。
    - 大海捞针:传递大量非结构化数据，提取最相关的信息或寻找关联/差别。
    -  调试和改进代码:需要审查并进一步调试、改进大量代码。
  - **速度和成本**:一般来说推理模型的推理时间较长，如果你对于时间和成本敏感，且任务复杂度不高，通用模型可能是更好的选择。
  - 当然你还可以在你的应用中**结合使用两种模型**: 使用**推理模型**完成**Agent的规划和决策**，使用**通用模型**完成**任务执行**。

### 4.2 如何有效地提示推理模型？
**推理模型在面对相对模糊的任务也能给出详尽且格式良好的响应**。因此，使用本节前面提到的一些提示词技巧（如指示模型“逐步思考”）**可能反而限制了模型的推理**。下面列举了一些适用于推理模型的提示技巧：
● **直接提问**：保持提示简洁、清晰，且明确任务限制。
● **避免思维链提示**：你无需提示推理模型“逐步思考”或“解释你的推理”，它们本身会进行深入的思考。
● **根据模型响应调整提示词**：

**直接提问**推理模型通常能够产生**良好的响应**，但如果你有更复杂精细的要求，可以在提示词中明确，比如**有明确的输入信息和输出要求时**，你可以通过**增加示例**明确这些信息，还可以**通过分隔符**帮助推理模型区分不同的信息模块。这个过程可以是重复多次的，不断尝试调整提示，让模型不断推理迭代，直到符合你的要求。

## 课时6 检索增强生成（RAG，Retrieval Augmented Generation）



### 1 什么是RAG

- **检索增强生成**包括三个步骤，建立**索引、检索、生成**。

  如果说大模型导游助理是一位志愿者，

  ​	准备“志愿者手册”	→ 	建立知识库**索引**，

  ​	志愿者查看资料	   →	系统在**检索**知识库，

  ​	志愿者基于检索到的资料充分思考并回答用户的问题 → **生成**答案。

### 2 RAG的实现原理

![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/alqvxlczbkggibwjnoji.png)

​				图3：大模型RAG基本工作流

#### 2.1 RAG主要由两个部分构成：

- **建立索引**： 

  →  清洗和提取原始数据(将 PDF、Docx等不同格式的文件解析为纯文本数据)； 

  →  将文本数据分割成更小的片段（chunk）

  →  将这些片段经过嵌入模型转换成向量数据（此过程叫做**embedding**），并将原始语料块和嵌入向量以键值对形式存储到**向量数据库**中，以便进行后续快速且频繁的搜索。

  *这就是建立索引的过程。*

- **检索生成**：

  →  系统会获取到用户输入，

  →  计算出用户的问题与向量数据库中的文档块之间的相似度，选择相似度最高的K个文档块（K值可以自己设置）作为回答当前问题的知识。

  →  **知识与问题**会合并到提示词模板中**提交给大模型**

  →  大模型给出回复。

  *这就是检索生成的过程。*

  

#### 3 RAG应用案例：阿里云AI助理

阿里云推出了阿里云AI助理，它背后的基本框架就是基于**通义千问大模型的RAG应用**，使用的知识库是阿里云各产品的使用文档。

#### 4 如何持续改进RAG应用效果

随着深入使用，你可能发现你的 RAG 应用可能只是能用了，但还有很多问题，比如：
● **问题比较抽象或者概念比较模糊**，导致大模型没有准确理解使用者的问题。例如，使用者问“兰州拉面去哪吃？”，使用者本来想问附近有没有卖兰州拉面的店铺；假如知识库搜索“兰州”在“拉面”之后，结果排序靠前的语料是位于“兰州”的拉面馆地址，而大模型告诉用户要去买飞机票或者火车票，就是答非所问了。通常我们用改造问题，让**使用者的问题更好理解的策略**来回避这种情况。
● **知识库没有检索到问题的答案，这有可能是由于语料数据没有做好整理就存入知识库，或者是检索策略有问题，参数需要调整导致的**。比如，**在采用“K个最相似文档块”作为回答的知识这个策略中**，如果K值比较小，那么最相似的K个文档块中可能并不包含能解答用户问题的有效知识，那么答案很可能就是错误的。例如，作为旅游手册的知识库中有大量文段是 兰州拉面如何制作的菜谱、兰州拉面的产地、兰州有哪些特产等，只有一两条信息描述了附近拉面馆的地址。那么当用户询问“兰州拉面怎么走？”时，知识库检索到的信息可能只是兰州拉面的选材、调味、烹饪方面的信息，而唯独没有检索到前方50米处有一家兰州拉面馆。用户也没有办法获得有效的答案。
● **缺少对答案做兜底验证的机制**，假设运气很好，志愿者不仅听懂了游客的问题，也正确查找到了附近最近的两家拉面馆的信息，但是志愿者的回答方式是“向北走200米就到了”。这有可能是一个正确的答案，但不是一个好的答案，实际考察过景区地形后我们可能会发现，志愿者北面是后海，你不太可能穿过湖面去一个地方。实际路径可能是：先向东走50米，再向北走绕过后海，走到湖对面去，才能走到正确位置。那这个“向北走200米...”的回答，从导航的角度就不能算是准确了。

![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/jafxhsulycvmxecthlwe.png)

#### 5.1 建立评测标准

为了持续改进我们的 RAG 应用，首要任务应当是构建一套严谨的**评测指标体系**，并邀请业务领域专家作为评测方共同参与评测工作，我们可以设置与我们业务相关的多种问题场景，系统性地检查一个RAG系统**反应快不快**，**回答准不准**，**有没有理解用户问题的意图**等方面。通过科学全面的评测，我们可以了解到系统在哪些地方做得好，哪些地方需要改进，从而帮助开发者让RAG系统更好的服务于业务需求。
RAG系统一般包括**检索**和**生成**两个模块，我们做评测时就可以从这两个模块分别入手建立**评价标准和实施方法**，当然你也可以**用最终效果为标准，建立端到端的评测**。在评测指标设计上，我们主要**考察检索模块**的准确性，如**准确率、召回率、F1值**等等；在**生成模块**，我们主要考察生成**答案的价值，如相关性、真实性**等等。
我们可以引入业界认可的一些通用评估策略，比如，你可以参考Ragas提及的评测矩阵指南，你也可以建立一些自己的评测指标，这些评测方法将会有助于你量化和改进每一个子模块的表现

#### 5.2 改造一：提升索引准确率

● **优化文本解析过程**
在构建知识库的时候，我们首先需要正确的从文档中提取有效语料。因此，**优化文本解析的过程往往对提升RAG的性能有很大帮助**。例如，从网页中提取有效信息时，我们需要判断哪些部分应该被去掉（比如页眉页脚标签），哪些部分应该被保留（比如属于网页内容的表格标签）。
● **优化chunk切分模式**
Chunk就是数据或信息的一个小片段或者区块。当你在处理大量的文本、数据或知识时，如果你一次性全部交给大模型来阅读和处理，效率是非常低的。所以，我们把它们切分成更小、更易管理的部分，这些部分就是chunk。每个chunk都包含了一些有用的信息，这样当系统根据用户的问题寻找某个知识时，它可以迅速定位到与答案相关信息的chunk，而不是在整个数据库中盲目搜索。因此，通过精心设计的**chunk切分策略，系统能够更高效地检索信息**，就像图书馆里按照类别和标签整理书籍一样，使得查找特定内容变得容易。**优化chunk切分模式，就能加速信息检索、提升回答质量和生成效率**。具体方法有很多：

1. **利用领域知识**：针对特定领域的文档，利用领域专有知识进行更精准的切分。例如，在法律文档中识别段落编号、条款作为切分依据。
2. **基于固定大小切分**：比如默认采用128个词或512个词切分为一个chunk，可以快速实现文本分块。缺点是忽略了语义和上下文完整性。
3. **上下文感知**：在切分时考虑前后文关系，避免信息断裂。可以通过保持特定句对或短语相邻，或使用更复杂的算法识别并保留语义完整性。最简单的做法是切分时保留前一句和后一句话。你也可以使用自然语言处理技术识别语义单元，如通过句子相似度计算、主题模型（如LDA）或BERT嵌入聚类来切分文本，确保每个chunk内部语义连贯，减少跨chunk信息依赖。通义实验室提供了一种文本切割模型，输入长文本即可得到切割好的文本块，详情可参考：中文文本分割模型。

- **句子滑动窗口检索**
  这个策略是通过**设置window_size（窗口大小）来调整提取句子的数量**，当用户的问题匹配到一个chunk语料块时，通过窗函数**提取目标语料块的上下文，而不仅仅是语料块本身**，这样来获得更完整的语料上下文信息，提升RAG生成质量。
- <img src="https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/wkdxvemgdhkebukzjquv.png" alt="img" style="zoom: 25%;" />

**● 自动合并检索**
这个策略是将文档分块，建成一棵语料块的树，比如1024切分、512切分、128切分，并构造出一棵文档结构树。当应用作搜索时，如果同一个父节点的多个叶子节点被选中，则返回整个父节点对应的语料块。从而确保与问题相关的语料信息被完整保留下来，从而大幅提升RAG的生成质量。实测发现这个方法比句子滑动窗口检索效果好。

![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/affugwhwbkxpxmrydpzy.png)

**● 选择更适合业务的Embedding模型**
经过切分的语料块在提供检索服务之前，我们需要把chunk语料块由原来的文本内容转换为机器可以用于比对计算的一组数字，即变为Embedding向量。我们通过Embedding模型来进行这个转换。但是，由于不同的Embedding模型对于生成Embedding向量质量的影响很大，好的Embedding模型可以提升检索的准确率。
比如，针对中文检索的场景，我们应当选择在中文语料上表现更好的模型。那么针对你的业务场景，你也可以建议你的技术团队做Embedding模型的技术选型，挑选针对你的业务场景表现较好的模型。
**● 选择更适合业务的ReRank模型**
除了优化生成向量的质量，我们还需要同时优化做**向量排序的ReRank模型**，好的ReRank模型会让更贴近用户问题的chunks的排名更靠前。因此，我们也可以挑选能让你的业务应用表现更好的ReRank模型。

比如： 关注度高、下载量大的模型，BAAI/BGE系列模型，阿里通用文本向量 API 服务

**● Raptor 用聚类为文档块建立索引**
还有一类有意思的做法是采用**无监督聚类来生成文档索引**。这就像通过文档的内容为文档自动建立目录的过程。假如志愿者拿到的文本资料是没有目录的，志愿者一页一页查找资料必然很慢。因此，可以将词条信息聚类，比如按照商店、公园、酒吧、咖啡店、中餐馆、快餐店等方式进行分组，建立目录，再根据汉语拼音字母来排序。这样志愿者来查找信息的时候就可以更快速地进行定位。

<img src="https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/wjvjshaaiatkczgvfvfa.png" alt="img" style="zoom:25%;" />

#### **5.3 改造二：让问题更好理解**

我们希望做到能让人们**通过口语对话**来使用大模型应用。然而，人们在口语化表达自己的目的和意图时，往往会出现一些问题。比如，**问题过于简单**含糊出现了**语义混淆**，导致大模型理解错误；问题的要素非常多，而用户又讲得太少，只能在反复对话中不断沟通补全；问题涉及的知识点超出了大模型训练语料，或者知识库的覆盖范围，导致大模型编造了一些信息来回答等等。所以，我们期望能在用户提问的环节进行介入，让大模型能更好的理解用户的问题。针对这个问题进行尝试的论文很多，提供了很多有意思的实现思路，如Multi-Query、RAG-Fusion、Decomposition、Step-back、HyDE等等，我们简要讲解一下这些方法的思路。
● Enrich 完善用户问题
我们首先介绍一种比较容易想到的思路，**让大模型来完善用户的原始问题**，产生一个更利于系统理解的完善后的用户问题，再让后续的系统去执行用户的需求。**通过用大模型对用户的问题进行专业化改写**，特别是加入了知识库的支持，我们可以生成更专业的问题。下图展现了一种理想的对用户问题的Enrich流程。我们通过多轮对话逐步确认用户需求。
❖ **一种理想的通过多轮对话补全需求的方案**。该设想是通过大模型多次主动与用户沟通，不断收集信息，完善对用户真实意图的理解，补全执行用户需求所需的各项参数。如下图所示。![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/ujjcdmdurmpktsujmyst.png)

❖ **让大模型转述用户问题，再进行RAG问答**。参考“指令提示词”的思路，我们可以让大模型来转述用户的问题，将用户的问题标准化，规范化。这里我们可以**提供一套标准提示词模板，提供一些标准化的示例，也可以用知识库来增强**。我们的主要目的是规范用户的输入请求，再生成RAG查询指令，从而提升RAG查询质量。

![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/xbrnvfntvmeczzfkyfia.png)

❖ **让用户补全信息辅助业务调用。**有一些应用场景需要大量的参数支撑，（比如订火车票需要起点、终点、时间、座位等级、座位偏好等等），我们还可以进一步完善上面的思路，**一次性告诉用户系统需要什么信息，让用户来补全**。首先，**需要准确理解用户的意图**，实现意图识别的手段很多，如使用向量相似度匹配、使用搜索引擎、或者直接大模型来支持。其次，**根据用户意图选择合适的业务需求模板**。接着，**让大模型参考业务需求模版来生成一段对话发给用户，请求用户补充信息**。这时，如果用户进行了信息完善，那么大模型就可以基于用户的回复信息结合用户的请求来生成下一步的行动指令，整个系统就可以实现应用系统自动帮助用户订机票、订酒店，完成知识库问答等应用形式。

![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/nvytdtqbrlcedyzmhtvx.png)

Enrich的方法介绍了一种**大模型向用户多次确认需求来补全用户想法的做法**。自此，我们假设已经获得了补全过的用户需求，但是由于用户面对的现实问题千变万化，而系统或RAG的知识可能会滞后，对用户问题的理解多少存在一些偏差，我们还可以继续对整个系统进行强化，接下来我们继续介绍“如何让系统更好地理解用户的问题”。

**● Multi-Query 多路召回**
多路召回的方法不是让大模型进行一次改写然后反复向用户确认，而是**让大模型自己解决如何理解用户的问题**。所以我们首先一次性改写出多种用户问题，让大模型根据用户提出的问题，从多种不同角度去生成有一定提问角度或提问内容上存在差异的问题。让这些存在差异的问题作为**大模型对用户真实需求的猜测**，然后**再把每个问题分别生成答案，并总结出最终答案**。

![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/yinnwegyrbxtftcuogdo.png)

~~~
例如：用户问“烤鸭店在哪里？”，大模型会生成：

~~~

![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/zoaaanmvzdlxqshyiksz.png)

**● RAG-Fusion 过滤融合**
在经过多路召回获取了各种语料块之后，并不是将所有检索到的语料块都交给大模型，而是先进行一轮筛选，给检索到的语料块进行去重操作，然后按照与原问题的相关性进行排序，再将语料块打包喂给大模型来生成答案。

![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/dqzietqxfkadmxssstwt.png)

经过**去重复语料筛选**，节省了传递给大模型的tokens数量，再经过排序，将更有价值的语料块传递给大模型，从而提升答案的生成质量。

**● Step Back 问题摘要**
让大模型先对问题进行一轮抽象，从大体上去把握用户的问题，获得一层高级思考下的语料块。

**● Decomposition 问题分解**
这个策略讲究细节，有点像提示词工程中的CoT（Chain of Thoughts，思维链）的概念，是把用户的问题拆成一个一个小问题来理解，或者可以说是RAG中的CoT。

**● HyDE 假设答案**
这个策略让大模型先来根据用户的问题生成一段假设答案，然后用这段假设的答案作为新的问题去文档库里匹配新的文档块，再进行总结，生成最终答案。
好比志愿者听到用户的问题“推荐一家烤鸭店”，第一时间想到了“全聚德烤鸭店不错，我前两天刚吃过！”，接下来，志愿者按照自己的思路找到了全聚德烤鸭店的地址，并给用户讲解如何走过去。

#### 5.4 改造三：改造信息抽取途径
**Corrective Retrieval Augmented Generation (CRAG)是一种改善提取信息质量的策略**：如果通过知识库检索得到的信息与用户问题相关性太低，我们就**主动搜索互联网**，将网络搜索到的信息与知识库搜索到的信息合并，再让大模型进行整理给出最终答案。在工程上我们可以有两种实现方式：

1. **向量相似度**，我们用检索信息得到的向量相似度分来判断。判断每个语料块与用户问题的相似度评分，是否高过某个阈值，如果搜索到的语料块与用户问题的相似度都比较低，就代表知识库中的信息与用户问题不太相关；
2. **直接问大模型**，我们可以先将知识库检索到的信息交给大模型，让大模型自主判断，这些资料是否能回答用户的问题

#### 5.5 改造四：回答前反复思考
**Self-RAG，也称为self-reflection**，是一种通过在应用中设计反馈路径实现自我反思的策略。基于这个思想，我们可以让应用问自己三个问题：
‒ **相关性**：我获取的这些材料和问题相关吗？
‒ **无幻觉**：我的答案是不是按照材料写的来讲，还是我自己编造的？
‒ **已解答**：我的答案是不是解答了问题？

![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/jhviikfctclmrgggyrhy.png)

#### 5.6 改造五：从多种数据源中获取资料
这个策略涉及系统性的**改造数据的存储和获取环节**。传统RAG我们**只分析文本文档**，我们把文档作为字符串存在向量数据库和文档数据库中。但是现实中的知识还有**结构化数据**以及**图知识库**。因此，有很多工作者在研究从数据库中通过NL2SQL的方式直接获取与用户问题相关的数据或统计信息；从GraphDB中用NL2Cypher（显然这是在用Neo4J）获取关联知识。这些方法显然将给RAG带来更多新奇的体验。

**● 从数据库中获取统计指标**
大模型可以将用户问题转化为SQL语句去数据库中检索相关信息，这个能力就是**NL2SQL**（**Natural Language to SQL**）。如果搜索的问题比较简单，只有单表查询，并获取简单的统计数据如求和、求平均等等，大模型还能稳定地生成正确SQL。如果问题比较复杂涉及多表联合查询，或者涉及复杂的过滤条件，或复杂的排序计算公式，大模型生成SQL的正确性就会下降。我们一般**用Spider榜单来评测大模型生成SQL的性能**。合理构造提示词**调用Qwen-Max生成SQL**，或者使用**SFT微调小模型**如Qwen-14B来生成SQL，都可以获得可满足应用的NL2SQL能力。



<img src="https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/gurehymanpagljxgvhqx.png" alt="img" style="zoom:33%;" />



● 从知识图谱中获取数据
**Neo4j是一款图数据库引擎**，可以为我们提供知识图谱构建和计算服务。在知识图谱上，我们调用各种图分析算法，如标签传播、关键节点发现等等，可以快速检索多度关联关系，挖掘隐藏关系。

![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/yrtsblsqjrobnolsdlmt.png)

### 5.7 总结

![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/moiqpihulxuvpigtbbot.png)

## 课时8 微调

### 1 关于“微调”，你应该知道的
#### 1.1 什么是微调

**大模型微调**就像给一个已经训练好的、很聪明的学生（大模型）进行**针对性补习**，**让大模型更懂你的专业领域、更符合你的特定需求**。例如，你需要训练学生能表演话剧，扮演医生、律师等特定角色。或者，你提供给学生关于你们公司业务系统的大量开发手册，需要学生快速地学习，然后就能加入你的开发团队，优化你们的系统了。
核心思想：在预训练的基础上，使用特定领域的数据对模型进行进一步的训练，从而让模型更擅长处理你想要解决的问题，也就是说，**让大模型更懂你**。

#### 1.2 微调能实现什么

**● 风格化（适应特定领域风格）**：
假设你有一个通用的大模型，它对各种话题都有所了解。但你想要让它作为医疗专家，专职回答医疗问题：即不仅可以理解病患的问题，还可以通过一两句话就能切中要害、直指问题并给出方案。
那么你可以通过微调大模型，用大量的医学文献和医疗病例对其进行训练，从而让大模型更准确地理解医学术语，给出专家建议。
**● 格式化（掌握特定系统接口）：**
假设你需要开发一个智能助理，对接一个很复杂的系统，这个系统具有诸多业务接口和复杂的API规范。根据之前的学习，你可能会想到以下方案：
采用RAG的方案，可能的问题：由于Chunk切分的方法可能会把较复杂的API规范切成几段，大模型可能无法获得完整的API规范。
把全部的API信息一次性塞给大模型，可能的问题：大模型允许输入的上下文Tokens很可能会被占满，大模型仍然有可能没有看到与任务有关的API规范。
使用一个支持200万tokens以上的大模型服务，如果每次用户请求服务时，系统后台都要把整本API文档交给大模型服务，以实现一些“日常小任务”，这又会造成极大的资源浪费。而且，过大的提示词也会导致系统的响应速度下降，导致用户体验变差。
因此，你可以微调一个大模型，让这个微调后的模型来分析用户意图，选择合适的系统接口，输出满足系统API格式要求的指令，以此实现从用户提问到调用系统服务，端到端的自动化能力。

#### 1.3 为什么要微调

- **提高效率和降低成本**：
  你可能在使用Qwen-72B-chat模型来处理某个文本分类任务。由于模型参数量较大，文本分类的准确率非常高，但同样因为参数量较大，模型的推理成本和耗时都比较高。为了达到近似的效果，并且降低推理成本和耗时，你可以直接使用Qwen-1.8B-chat模型来处理这个分类任务，尽管推理成本和耗时低很多，但分类准确度可能也会低很多。此时，你可以尝试通过一个文本分类数据集对其进行微调，让微调后的Qwen-1.8B-chat模型在分类任务中的表现接近Qwen-72B-chat。虽然牺牲了可接受范围的准确度，但是成本和推理速度获得了极大改善。

#### 1.4 微调的关键

● **需要特定领域的高质量数据**：只有收集到高质量的数据，微调后的模型才可能会表现出色，但是高质量的数据往往并不容易获得，收集的过程可能会带来成本和时间上的挑战。
● **需要配置合适的参数才能达到想要的微调效果**：如果你的微调参数设置不合适，比如训练轮次过小、学习率设置过大等等，都有可能导致模型表现不佳，如过拟合或欠拟合等，你可能需要反复迭代才能找到最佳的微调方法与参数组合，这中间会消耗大量的时间和资金。
	过拟合（Overfitting） 是机器学习中常见的现象，指的是模型在训练数据上表现非常好，但在测试数据或实际应用中表现很差。就好像一个学生只记住了题目的答案，但却无法理解题目的本质，无法灵活运用知识解决新问题。

​	欠拟合也是机器学习中常见的现象，指的是由于训练过程过于简单，导致模型在训练数据与测试数据上表现都不好。
总而言之，**大模型微调就像给模型进行个性化定制，可以帮助它更好地完成你的任务，但需要你投入时间和精力进行准备和训练。**

#### 1.5 如何进行微调

我们可以考虑使用RAG扩大知识库范围的方式来让大模型知道西红市第十实验小学的相关信息，但可能结果还是不够准确，信息还是不足，或者速度不满足要求。。不断微调大模型，更新大模型在西红市第十实验小学这个**专有领域的知识库**，

#### 1.5.1 业务决策

 你在决定是否在业务领域中使用微调大模型时，可以考虑多个因素以确保采用微调大模型的方法能带来预期的业务价值。

- **业务需求匹配度**

首先明确业务的具体需求，明确要通过大模型微调解决的具体业务问题和应用场景。可以先问自己以下问题：

- 你的任务是否需要复杂的语言理解或生成能力？例如，复杂的自然语言生成、意图识别等任务可能受益于大模型微调。
- 你的任务是否需要高度特定于某个领域或任务的语言能力？例如，法律文书分析、医学文本理解等领域任务。
- 当前的模型是否已能满足大部分需求？如果能满足，则可能不需要微调。
- 是否有具体的业务指标来衡量微调前后效果对比？比如微调后的大模型推送给客户的信息更加准确，从而降低投诉率。

- **数据可用性与质量**

微调需要足够的高质量领域特定数据。需要评估当前业务系统中是否能够提取出足够的标注数据用于训练，以及数据的质量、代表性是否满足要求。

- **合规与隐私**

业务工作者需要确保使用的数据符合法律法规要求，处理个人数据时遵循隐私保护原则，尤其是GDPR等国际和地区隐私法规。此外，还要关注模型偏见、过拟合、泛化能力不足等潜在风险，以及这些风险对业务可能造成的影响。

- **资源和技术可行性**

微调过程需要计算资源和时间成本，包括GPU资源、存储空间以及可能的专家人力成本。需要评估项目预算和资源是否允许进行有效微调。

团队是否具备微调大模型所需的技术能力和经验，或者是否有合适的合作伙伴提供技术支持。

总的来说，需要对微调的业务价值进行ROI分析，即进行成本效益分析，评估微调带来的商业价值是否超过其成本，包括直接经济效益和间接效益，如用户体验提升、品牌形象增强等。

#### 1.5.2 微调的流程





# 重点摘-大模型

## 1.人工智能

- 人工智能（AI）是一门使**机器**模拟**人类智能**过程的学科

### 1.1 人工智能子领域:

人工智能 AI: Artificial Intelligence

- **机器学期 ML: Machine Learning** 

  - ### 监督学习

    - **学习过程**：使用**有标记的数据**进行学习，即数据集中的每个样本都有对应的目标值（标签），算法通过学习输入特征和标签之间的映射关系，建立预测模型，当有新的输入数据时，模型能预测出相应的输出标签。
    - **例子**：以垃圾邮件分类为例，我们收集了大量的邮件样本，并人工标注出这些邮件是 “垃圾邮件” 还是 “正常邮件” 。将这些带有标签（垃圾邮件 / 正常邮件）的邮件数据输入到监督学习算法（比如朴素贝叶斯算法 ）中，算法会学习邮件内容（如关键词、发件人信息等特征）与标签之间的关系。当收到新邮件时，模型就能根据学到的关系预测该邮件属于垃圾邮件还是正常邮件。

  - ### 无监督学习

    - **学习过程**：处理的是**无标记的数据**，算法的目标是发现数据中隐藏的模式、结构或关系，比如对数据进行聚类，将相似的数据点**归为同一类**，没有预先设定的类别标签。
    - **例子**：除了上述让孩子观察水果分组的例子，在电商领域也很常用。电商平台拥有大量客户的购买记录数据，但没有对客户进行分类。通过无监督学习算法（如 K - 均值聚类算法 ），可以根据客户购买商品的种类、购买频率、消费金额等特征，将客户分成不同的群体。比如可能发现一部分客户经常购买高端电子产品，另一部分客户则偏好购买家居日用品，进而可以针对不同群体制定个性化的营销策略。

  - ### 强化学习

    - **学习过程**：智能体（agent）在环境中采取一系列行动，根据行动产生的**奖励（reward）信号**来学习最优策略，以最大化长期累积奖励。智能体在与环境的交互过程中不断试错，逐步调整自己的行为。
    - **例子**：以机器人下棋为例，机器人就是智能体，棋盘和对手构成了环境。机器人在每一步棋都有多种走法（行动），当它走出一步棋后，根据棋局的变化会得到一个奖励值（比如赢了获得正奖励，输了获得负奖励 ）。机器人通过不断和对手下棋（与环境交互），尝试不同的走法，根据得到的奖励来调整自己的下棋策略，逐渐学会在各种局面下选择最优的走法，以提高赢得比赛的概率。

​	三者对比总结

|  学习方式  | 数据标记情况     | 学习目标                           | 主要应用场景                                  |
| :--------: | ---------------- | ---------------------------------- | --------------------------------------------- |
|  监督学习  | 有标记数据       | 学习输入与输出的映射关系，进行预测 | 分类（如疾病诊断分类）、回归（如房价预测）    |
| 无监督学习 | 无标记数据       | 发现数据中的模式和结构             | 聚类分析（如客户分群）、降维（如图片压缩）    |
|  强化学习  | 根据行动获得奖励 | 学习最优行动策略                   | 游戏（如 AlphaGo 下棋）、机器人控制、资源管理 |



### 深度学习 DL -Deep Learning

### 监督学习、无监督学习、强化学习是机器学习中的学习范式，深度学习则是一种基于人工神经网络的技术架构

- 深度学习与监督学习
  - **结合方式**：在深度学习中运用监督学习时，会利用大量带有明确标签的数据对深度神经网络进行训练。以图像分类为例，通过卷积神经网络（CNN） ，将大量已标注好类别的图片数据输入网络，网络学习图片的特征与类别之间的映射关系。比如在 CIFAR - 10 数据集（包含 10 个类别共 6 万张彩色图像）上训练 CNN，网络会学习到如何根据图像中的像素信息、物体形状、颜色等特征，判断图像是飞机、汽车、鸟等 10 个类别中的哪一个。
  - **应用场景**：在自然语言处理领域的情感分析任务中，也会使用监督学习的深度学习模型。例如使用循环神经网络（RNN）及其变体 LSTM（长短期记忆网络），将大量标注好情感倾向（正面、负面、中性）的文本数据作为输入，模型学习文本中的词汇、语法、语义等特征与情感倾向的关系，从而能够对新的文本进行情感分类。
- 深度学习与无监督学习
  - **结合方式**：深度学习结合无监督学习时，通常是在没有明确类别标签的数据上，让深度神经网络自动学习数据的潜在特征和结构。**自编码器（Autoencoder）**就是一种典型的无监督深度学习模型，它由编码器和解码器两部分组成。编码器将输入数据压缩成低维表示，解码器再将低维表示还原回原始数据形式，通过最小化原始输入和重建输出之间的误差，使模型学习到数据的有效特征表示。
  - **应用场景**：在图像领域，自编码器可以用于图像降噪。将含有噪声的图像输入自编码器，模型学习到图像的干净特征表示后，在解码阶段输出去噪后的图像。此外，生成对抗网络（GAN）也是无监督深度学习的重要代表，它由生成器和判别器组成，生成器学习生成逼真的数据样本（如图片、音频），判别器判断样本是真实数据还是生成数据，二者相互对抗博弈，从而生成高质量的样本数据，可用于图像生成、数据增强等。
- 深度学习与强化学习
  - **结合方式**：深度强化学习是深度学习与强化学习相结合的产物，即使用深度神经网络来近似强化学习中的值函数或策略函数。以深度 Q 网络（DQN）为例，它将深度神经网络与 Q - 学习算法相结合，用神经网络来估计 Q 值（在某一状态下采取某一行动的长期累积奖励期望）。在训练过程中，智能体与环境进行交互，根据环境反馈的奖励和状态转移信息，不断更新神经网络的参数，以学习到最优策略。
  - **应用场景**：在游戏领域，AlphaGo 就是深度强化学习的典型应用，它使用卷积神经网络来表示围棋棋盘的状态，通过与大量棋局数据进行自我对弈，并结合强化学习算法，学习到了非常强大的围棋策略，战胜了人类顶尖棋手。此外，在自动驾驶中，智能车辆可以通过深度强化学习，学习在不同路况、交通信号等环境状态下的最优驾驶策略。

- ### 生成式人工智能 GAI

生成式人工智能（Generative Artificial Intelligence）是人工智能的一个重要分支，指能够自主生成新内容（如文本、图像、音频、视频等）的人工智能技术。它基于机器学习算法，尤其是深度学习模型，通过对大量已有数据的学习，捕捉数据的模式、结构和特征，进而**生成与训练数据相似的新样本**。

### 核心技术与模型

- **生成对抗网络（GAN）** ：由生成器（Generator）和判别器（Discriminator）组成。生成器负责生成新的数据样本，比如生成逼真的图像；判别器则对生成器生成的样本以及真实的样本进行判断，区分是真实数据还是生成数据。两者相互对抗博弈，生成器不断优化生成质量以骗过判别器，判别器不断提高鉴别能力，最终使生成器能够生成高度逼真的数据。例如，在图像生成中，GAN 可以生成以假乱真的人脸图像。
- **变分自编码器（VAE）** ：一种基于概率模型的生成式模型。它通过对输入数据进行编码，将其映射到一个潜在空间，然后从潜在空间中采样并通过解码器生成新的数据。VAE 在图像生成、数据压缩等领域有广泛应用，能够生成具有一定多样性的样本，比如生成不同风格的绘画作品。
- **Transformer 模型** ：在自然语言处理领域取得了巨大成功，基于 Transformer 架构的模型，如 GPT（Generative Pretrained Transformer）系列 ，通过在大规模文本数据上进行预训练，学习到语言的语法、语义和逻辑等模式，能够生成连贯、有意义的文本，像创作故事、回答问题、生成代码等

### 应用领域

- **内容创作**：在文本创作方面，生成式人工智能可以辅助作家、记者等创作文章、小说、新闻报道等，如自动生成故事梗概、新闻导语；在图像创作领域，设计师利用其快速生成设计草图、创意海报；在音乐创作上，能根据给定的风格、节奏等要求生成新的乐曲。
- **虚拟角色与场景构建**：游戏开发中，用于创建逼真的虚拟角色形象、丰富的游戏场景和关卡，降低开发成本和时间；在影视制作里，生成特效场景、虚拟演员，比如在一些科幻电影中，利用生成式人工智能技术创造出奇幻的外星场景。
- **数据增强**：在机器学习和深度学习模型训练时，如果训练数据量不足，生成式人工智能可以生成额外的相似数据，扩充数据集，提升模型的泛化能力。例如，在图像识别任务中，生成更多不同角度、光照条件下的图像样本。
- **医疗领域**：辅助生成医学图像，如生成模拟的 X 光、CT 图像，用于医学影像分析模型的训练；还可以生成个性化的治疗方案建议，通过学习大量病例数据，为医生提供参考。

### 面临的挑战与问题

- **质量与可靠性**：生成的内容可能存在逻辑错误、质量参差不齐的问题。例如，文本生成模型可能会生成一些语义不通顺或与事实不符的内容；图像生成模型生成的图像可能在细节上存在瑕疵，如人物手部结构不合理等。
- **伦理与安全问题**：恶意使用生成式人工智能可能带来严重的社会危害，如生成虚假新闻、伪造名人声音和形象进行诈骗，以及侵犯知识产权等问题。
- **计算资源需求**：训练生成式人工智能模型通常需要大量的计算资源和时间，像训练大规模的语言模型需要使用高性能的 GPU 集群，成本高昂。

## 2.大模型-Foundational Models

- 2021年，斯坦福大学   大模型-Foundational Models
- 2022年11月，OpenAI公司发布了ChatGPT
- 2023年3月，国内厂商纷纷发布各自研发的大语言模型产品。
- 2023年8月，阿里巴巴集团发布了通义千问系列开源大模型，并相继推出了7B（约70亿参数）、72B（约720亿参数）等不同参数规模的大语言模型版本。

### 2.1大模型分类

基础模型（大模型）： 大语言模型（LLM），多态模型

- #### 大语言模型（LLM）：

这类大模型专注于自然语言处理（NLP），旨在处理语言、文章、对话等自然语言文本。它们通常基于深度学习架构（如Transformer模型），经过大规模文本数据集训练而成，能够捕捉语言的复杂性，包括语法、语义、语境以及蕴含的文化和社会知识。

语言大模型典型应用: 包括文本生成、问答系统、文本分类、机器翻译、对话系统等。示例包括：GPT系列（OpenAI）：如GPT-3、GPT-3.5、GPT-4等。

Gemini（Google）：谷歌推出的大型语言模型，用于提供信息丰富的、有创意的文本输出。

通义千问（阿里云）：阿里云自主研发的超大规模的语言模型。

- #### 多模态模型：

多模态大模型能够同时处理和理解来自不同感知通道（如文本、图像、音频、视频等）的数据，并在这些模态之间建立关联和交互。它们能够整合不同类型的输入信息，进行跨模态推理、生成和理解任务。多模态大模型的应用涵盖视觉问答、图像描述生成、跨模态检索、多媒体内容理解等领域。

### 2.2 大模型的特点

-  ### 规模和参数量大

大模型拥有数**亿**甚至数**千亿**级别的参数。这些众多的参数就如同模型的 “知识神经元”，能帮助模型捕捉到数据中复杂的模式。比如，在处理自然语言时，大量的参数可以让模型更好地理解句子中词汇之间的微妙关系、语法结构以及语义内涵，从而能够生成丰富且准确的文本信息，像流畅的文章、精准的回答等。

- ### 适应性和灵活性强

大模型具备很强的适应不同任务的能力。它不需要为每个新任务都重新训练整个模型，通过微调（fine - tune）或者少样本学习的方式，就能高效地迁移到各种下游任务中。例如，原本训练好的大模型，只需用少量特定领域的数据进行微调，就可以应用于医疗诊断辅助、金融数据分析等不同领域的任务，展现出很强的跨领域工作能力。

- ### 广泛数据集的预训练

大模型在训练过程中，会使用大量多样化的数据集，涵盖语言、图像等多种类型的数据。通过在这些广泛的数据上进行预训练，大模型能够学习到通用的知识表示。比如，在语言方面，它能掌握不同风格、不同领域文本的特点；在图像方面，能识别各种物体的特征、场景的类型等，为后续处理各类具体任务奠定基础。

- ### 计算资源需求大

由于大模型规模庞大，参数众多，所以对计算资源的要求很高。这包括需要**大量的存储空间**来存储模型参数和训练数据；训练模型需要花费很长的时间；同时，训练过程中会**消耗大量的能量**，并且对硬件设施（如高性能的 GPU、充足的内存等）也有很高的要求，这些都使得大模型的训练和部署成本高昂。



### 2.3大模型工作过程

- 训练阶段 
- 推理阶段

### 2.3.1 大模型的训练

大模型的训练整体上分为三个阶段：

- **预训练**、
- **SFT（监督微调）**
- **RLHF（基于人类反馈的强化学习）**

###  1）预训练（Pretraining）

- **训练目标**：在大规模无标注的通用数据上进行训练，让模型学习到广泛的语言知识、语义理解和语法规则等，捕捉数据中潜在的模式和特征，从而具备一定的语言理解和生成能力基础。

- **数据使用**：使用海量的文本数据，这些数据来源多样，涵盖互联网文本、书籍、论文、新闻等各个领域，数据通常不带有特定任务的标注信息。

- **训练方式**：一般采用自监督学习方法，常见的如掩码语言模型（Masked Language Model，MLM）任务，在输入文本中随机掩盖一些单词，让模型预测被掩盖的部分；或者采用自回归语言模型（Autoregressive Language Model），根据前文预测下一个单词。

- **例子**：以 GPT-3 为例，它在训练时使用了海量的互联网文本数据，通过自回归的方式进行预训练。模型学习到了丰富的语言知识，比如不同词汇的语义关系、各种句式结构的用法等，能够生成较为流畅的文本，但此时它并没有针对特定任务进行优化，对一些专业领域或特定需求的回答可能不够精准。

  预训练的过程类似于从婴儿成长为中学生的阶段，在这个阶段我们会学习各种各样的知识，我们的语言习惯、知识体系等重要部分都会形成；对于大模型来说，在这个阶段它会学习各种不同种类的语料，学习到语言的统计规律和一般知识。但是大模型在这个阶段只是学会了补全句子，**却没有学会怎么样去领会人类的意图**，假设我们向预训练的模型提问：“埃菲尔铁塔在哪个国家？”模型有可能不会回答“法国”，而是根据它看到过的语料进行输出：“东方明珠在哪个城市？”这显然不是一个好的答案，因此我们需要让它能够去遵循人类的指示进行回答，这个步骤就是SFT（监督

### 2） 监督微调（Supervised Fine - Tuning，SFT）

- **训练目标**：在预训练模型的基础上，针对特定任务或应用场景，使用有标注的示例数据对模型进行进一步训练，使模型在特定任务上的性能得到提升，能够更好地满足实际应用需求。

- **数据使用**：收集与特定任务相关的标注数据，例如在问答任务中，准备大量的问题 - 答案对数据；在情感分类任务中，标注好正面、负面或中性情感的文本数据。

- **训练方式**：采用监督学习的方式，将标注数据输入预训练模型，以最小化预测结果与标注标签之间的差异为目标，调整模型的参数。

- **例子**：假设已经有一个经过预训练的语言模型，现在要将其应用于客服问答系统。那么可以收集大量常见的客服问题以及对应的标准回答，组成有标注的数据集。将这些数据输入预训练模型进行监督微调，模型就会学习到如何针对客服领域的问题给出更准确、专业的回答，相较于预训练阶段，在客服问答任务上表现会更出色。

  SFT的过程类似于从中学生成长为大学生的阶段，在这个阶段我们会学习到专业知识，比如金融、法律等领域，我们的头脑会更专注于特定领域。对于大模型来说，在这个阶段它可以学习各种人类的对话语料，甚至是非常专业的垂直领域知识，在监督微调过程之后，它可以**按照人类的意图去回答专业领域的问题**。这时候我们向经过SFT的模型提问：“埃菲尔铁塔在哪个国家？”模型大概率会回答“法国”，而不是去补全后边的句子。这时候的模型已经可以按照人类的意图去完成基本的对话功能了，但是模型的回答有时候可能并不符合人类的偏好，它可能会输出一些涉黄、涉政、涉暴或者种族歧视等言论，这时候我们就需要对模型进行RLHF（基于人类反馈的强化学习）。

### 3 ）基于人类反馈的强化学习（Reinforcement Learning from Human Feedback，RLHF）

- **训练目标**：在监督微调后的模型基础上，通过人类反馈作为奖励信号，利用强化学习算法进一步优化模型，使模型生成的结果更符合人类的偏好、价值观和使用习惯。
- **数据使用**：收集人类对模型生成结果的反馈数据，例如人类对模型给出的多个回答进行排序，或者直接给出评价分数。
- **训练方式**：将模型视为强化学习中的智能体，模型生成的结果作为智能体的行动，人类反馈作为奖励信号。通过强化学习算法（如近端策略优化算法，Proximal Policy Optimization，PPO），不断调整模型的参数，使模型生成的结果能够获得更高的奖励。
- **例子**：还是以客服问答系统为例，监督微调后的模型可以回答客服问题，但回答的质量和符合用户期望的程度可能还不够。此时，让人工标注员对模型针对同一问题给出的多个回答进行评分，模型将这些评分作为奖励信号，通过 RLHF 不断优化自身，逐渐学会生成更能让用户满意的回答，比如回答更简洁明了、更具针对性等。

RLHF的过程类似于从大学生步入职场的阶段，在这个阶段我们会开始进行工作，但是我们的工作可能会受到领导和客户的表扬，也有可能会受到批评，我们会根据反馈调整自己的工作方法，争取在职场获得更多的正面反馈。对于大模型来说，在这个阶段它会针对同一问题进行多次回答，人类会对这些回答打分，大模型会在此阶段学习到如何输出分数最高的回答，使得回答更符合人类的偏好。

### 三者关系总结

预训练为模型打下了广泛的语言理解和生成能力基础；监督微调让模型针对特定任务进行优化，提高在具体任务上的表现；RLHF 则基于人类反馈进一步优化模型，使模型输出更贴合人类的需求和偏好，三者层层递进，共同提升大语言模型的性能和实用性。



### 2.3.2 推理过程 - 处理提示词

用户在可以使用自然语言与大模型交流，用户的文本就是“提示词”。大模型处理提示词的工作流程可以分为两部分，第一部分是分词化与词表映射，第二部分为生成文本。

#### 1）第一部分： 分词化（Tokenization）与词表映射

**分词化**（**Tokenization**）是自然语言处理（NLP）中的重要概念，它是将段落和句子分割成更小的**分词**（**token**）的过程。 将一个句子分解成更小的、独立的部分可以帮助计算机理解句子的各个部分，以及它们在上下文中的作用。分词化有不同的粒度分类：

- 词粒度（Word-Level Tokenization）分词化，如上文中例子所示，适用于大多数西方语言，如英语。
- 字符粒度（Character-Level）分词化是中文最直接的分词方法，它是以单个汉字为单位进行分词化。
- 子词粒度（Subword-Level）分词化，它将单词分解成更小的单位，比如词根、词缀等。这种方法对于处理新词（比如专有名词、网络用语等）特别有效，因为即使是新词，它的组成部分（子词）很可能已经存在于词表中了。

例如：对于提示词 “介绍一下巴黎的著名景点”，大模型可能会将其切分为 “介绍” “一下” “巴黎” “的” “著名” “景点” 等词 。

**词表映射**：大模型内部有一个预先构建好的词表，包含了在训练过程中遇到的各种词或子词。分词完成后，模型会将每个切分后的单元与词表进行匹配，为它们分配唯一的编号（也叫索引）。

每一个token都会通过预先设置好的词表，映射为一个 token id，这是token 的“身份证”。一句话最终会被表示为一个元素为token id的列表，供计算机进行下一步处理。

假设词表中 “巴黎” 的编号是 123，“景点” 的编号是 456 ，那么经过词表映射后，原本的提示词就转化为了一串数字编号，如 “[编号 1，编号 2，123，编号 4，编号 5，456]” 。通过这种方式，将人类可读的文本转化为计算机能够高效处理的数字向量形式，便于模型后续进行计算和分析。

例子1： 

提示词：“介绍一下巴黎的著名景点”

分词化: [介绍，一下，巴黎，的，著名，景点]  

词表映射：  [4669，64938，123，358，279，456]    **共6个token**

例子2： 

提示词： I really enjoy learning ACA certification. I hope to pass the exam very soon!

分词化:  [I、really、enjoy、learning、ACA、certification、.、I、hope、to、pass、the、exam、very、soon、!]

词表映射：[40、2167、4669、6832、64938、27606、13、358、3900、311、1494、279、7006、1602、5135、0]   **共16个token**

#### 2） 第二部分：**生成文本**

- **初始状态设定**：模型在接收到经过分词化与词表映射后的提示词向量表示后，会根据自身的架构和训练得到的参数，设定初始的隐藏状态等内部参数。以基于 Transformer 架构的大模型为例，会通过多层的自注意力机制对输入的向量进行特征提取和交互，捕捉提示词中各个部分之间的语义关系，比如 “巴黎” 和 “景点” 之间的所属关系 。
- **逐词生成**：模型会根据初始状态和输入信息，**计算下一个最有可能出现的词的概率分布**。它会参考词表中每个词的可能性，选择概率较高的词作为输出。比如在回答 “介绍一下巴黎的著名景点” 时，模型可能首先计算出 “埃菲尔铁塔” 这个词在当前语境下出现的概率较高，就会选择它作为生成的第一个词。**然后，将已经生成的词（同样经过分词化与词表映射处理）与原始提示词一起，作为新的输入再次进入模型，继续计算下一个词的概率分布，重复这个过程**，不断生成后续的词，逐步构建出完整的回答，如 “巴黎的著名景点有埃菲尔铁塔，它是巴黎的标志性建筑……” 。
- **结束条件判断**：模型在生成文本时，会有一定的结束条件。可能是达到了预设的最大文本长度，比如设定生成的回答最多不超过 200 个词；也可能是生成了特定的结束标记，像遇到 “<END>” 这样的特殊符号，就停止文本生成，最终将生成的词序列转换回人类可读的文本形式，呈现给用户。

![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/ubfgivcuiajvvmtphfzk.png)

## 3.阿里云大模型

**通义千问**和**通义万相**是阿里巴巴通义系列产品中的基础模型。

- **通义千问**

阿里云自主研发的超大规模的语言模型。它支持多种语言，还能处理多种分辨率和规格的图像，实现多语言多模态理解。

- **通义万相**

是阿里云自主研发多模态图像和视频生成模型，可提供AI艺术创作，可支持文生图、图生图、图生视频、虚拟模特、个人写真等多场景的图片和视频创作能力。


## 重点摘 -RAG

RAG 即检索 - 增强 - 生成（Retrieval-Augmented Generation），是一种结合了信息检索与文本生成的技术框架，旨在通过动态引入外部知识提升生成模型的效果。

### 核心原理

- **检索（Retrieval）**：当用户提出问题时，系统会从外部知识库（如文档、数据库、网页等）中检索相关段落或信息。例如，用户询问 “量子计算机的最新进展有哪些？”，RAG 系统会使用语义搜索或关键词匹配等方式，从预先构建的外部知识库（如专业的科学数据库、最新的科研论文网站等）中查找最相关的文档或信息片段。

- **增强（Augmentation）**：将检索到的信息与原始输入结合，为生成模型提供上下文支持。比如，把从知识库中检索到的关于量子计算机最新进展的相关文档片段，拼接在用户问题 “量子计算机的最新进展有哪些？” 之后，形成增强的输入，如 “量子计算机的最新进展有哪些？[具体的文档片段内容]”。

- **生成（Generation）**：基于增强后的上下文，生成模型（如 GPT、BART 等）输出更准确、信息丰富的回答。例如，经过增强的输入进入生成模型后，模型会根据这些信息生成类似于 “量子计算机在近期取得了重要进展，某研究团队成功实现了更高的量子比特数，提升了计算精度和速度；还有团队在量子纠错技术方面有了新的突破，降低了量子计算中的错误率……” 这样的回答。

  ![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/alqvxlczbkggibwjnoji.png)

![image-20250830153041007](D:\SwirebevUser\chennl\AppData\Roaming\Typora\typora-user-images\image-20250830153041007.png)

如图所示，RAG主要由两个部分构成：
**建立索引**：首先要清洗和提取原始数据，将 PDF、Docx等不同格式的文件解析为纯文本数据；然后将文本数据分割成更小的片段（chunk）；最后将这些片段经过嵌入模型转换成向量数据（此过程叫做embedding），并将原始语料块和嵌入向量以键值对形式存储到向量数据库中，以便进行后续快速且频繁的搜索。这就是建立索引的过程。
**检索生成**：系统会获取到用户输入，随后计算出用户的问题与向量数据库中的文档块之间的相似度，选择相似度最高的K个文档块（K值可以自己设置）作为回答当前问题的知识。知识与问题会合并到提示词模板中提交给大模型，大模型给出回复。这就是检索生成的过程。
提示词模板类似于：请阅读：{知识文档块}，请问：{用户的问题}

### RAG和传统的信息检索技术有什么区别？

RAG（检索 - 增强 - 生成）与传统信息检索技术的核心差异，体现在**目标定位、工作流程、输出形式、知识处理能力**等多个维度，本质是 “从‘**找信息**’到‘**用信息生成答案**’” 的技术升级。

- ### 一、核心目标：“找信息” vs “生成可用答案”

  传统信息检索技术的核心目标是 **“精准定位并返回相关信息”**，本质是 “信息筛选工具”；而 RAG 的核心目标是**“基于检索到的信息，生成符合人类理解习惯的、结构化的可用答案”**，本质是 “信息加工 + 答案生成系统”。

  - **传统信息检索示例**：
    当你在学术数据库（如知网）中搜索 “2024 年人工智能医疗应用进展” 时，系统会根据关键词匹配、文献相关性排序，返回一系列相关的论文标题、摘要或全文链接 —— 你需要自己点击查看、阅读、提炼信息，才能得到 “2024 年 AI 在肿瘤诊断中的应用突破” 这类结论。
  - **RAG 示例**：
    若使用基于 RAG 的医疗问答系统提问同样问题，系统会先检索数据库中的 2024 年最新文献 / 行业报告，自动提取关键信息（如 “某团队用 AI 影像模型将肺癌早期诊断准确率提升至 92%”“AI 药物研发缩短某靶向药临床试验周期 30%”），再整合这些信息生成连贯回答：
    “2024 年人工智能在医疗领域的应用有显著进展：在诊断层面，AI 影像模型对肺癌早期诊断的准确率已达 92%，比传统人工诊断效率提升 3 倍；在药物研发领域，AI 算法帮助某靶向药的临床试验周期缩短 30%，加速了新药落地。”
    你无需手动筛选和提炼，直接获得可直接使用的答案。

  ### 二、工作流程：“单一检索” vs “检索 - 增强 - 生成三步闭环”

  传统信息检索是 **“输入→检索→返回信息” 的二步流程 **，仅负责 “找到信息”；而 RAG 是 **“输入→检索→信息增强→生成答案” 的三步闭环 **，新增了 “信息整合” 和 “自然语言生成” 环节，实现 “信息到答案的转化”。

  #### 1. 传统信息检索流程（以搜索引擎为例）：

  1. **用户输入**：“2024 年中国新能源汽车销量数据”
  2. **检索匹配**：搜索引擎通过关键词匹配（如 “2024”“中国”“新能源汽车”“销量”），从网页库中筛选相关页面，按相关性排序；
  3. **返回结果**：呈现一系列网页链接（如 “乘联会 2024 年新能源汽车销量报告”“中国汽车工业协会官网数据”），用户需自主点击查看具体数据。

  #### 2. RAG 流程（以新能源汽车问答系统为例）：

  1. **用户输入**：“2024 年中国新能源汽车销量数据”
  2. **检索（Retrieval）**：系统从结构化数据库（如乘联会官方数据、行业年报）中，精准检索到 “2024 年中国新能源汽车累计销量达 1200 万辆，同比增长 25%，其中纯电动车占比 68%，插混车型占比 32%” 等关键信息；
  3. **增强（Augmentation）**：将检索到的结构化数据与用户问题结合，形成 “增强输入”：“用户询问 2024 年中国新能源汽车销量数据，已知数据：累计 1200 万辆，同比 + 25%，纯电占 68%，插混占 32%”；
  4. **生成（Generation）**：生成模型（如 GPT-4 小型化模型）基于增强输入，将数据转化为自然语言回答：“2024 年中国新能源汽车销量表现亮眼，累计销量达 1200 万辆，较 2023 年同比增长 25%。从车型结构看，纯电动汽车仍是主流，占比 68%；插电式混合动力车型增速较快，占比提升至 32%。”

  ### 三、输出形式：“信息片段 / 链接” vs “结构化自然语言”

  传统信息检索的输出是 **“原始信息的集合”**（如链接、文档片段、数据表格），需要用户二次加工；而 RAG 的输出是**“经过整合的、人类可直接理解的自然语言文本”**（如段落、列表），无需额外处理。

  - **传统信息检索输出**：
    搜索 “如何制作提拉米苏”，返回的是 10 个不同的食谱网页链接，每个链接包含不同的步骤描述（有的先打蛋黄，有的先泡咖啡），你需要对比、筛选后才能确定自己的做法。
  - **RAG 输出**：
    向 RAG 美食助手提问同样问题，输出是结构化步骤：
    “制作提拉米苏的核心步骤如下：1. 准备材料（马斯卡彭芝士 200g、蛋黄 3 个、细砂糖 50g、浓缩咖啡 100ml、手指饼干 100g、可可粉适量）；2. 蛋黄加细砂糖打发至发白，加入马斯卡彭芝士搅拌均匀；3. 手指饼干快速蘸取浓缩咖啡，铺在容器底部，倒入芝士糊，重复 1-2 层；4. 冷藏 4 小时以上，食用前撒可可粉即可。”
    输出直接是可执行的步骤，无需用户筛选整合。

  ### 四、知识处理能力：“静态匹配” vs “动态整合与理解”

  传统信息检索依赖 **“关键词 / 规则的静态匹配”**，无法理解语义关联，也无法处理复杂问题；而 RAG 结合了 “检索的精准性” 和 “生成模型的语义理解能力”，能处理多维度、需要逻辑整合的问题。

  - **传统信息检索的局限**：
    若你提问 “为什么 2024 年中国新能源汽车销量增长比 2023 年慢？”，传统搜索引擎会优先匹配 “2024 新能源销量增长慢” 的关键词，返回的可能是零散的网页（如 “2024 新能源补贴退坡”“2023 年基数高”），但无法将这些原因整合为逻辑连贯的分析。
  - **RAG 的优势**：
    RAG 系统会先检索 “2024 年新能源销量增速 25%、2023 年增速 35%”“2024 年补贴政策退坡”“2023 年因购置税优惠出现提前消费” 等信息，再通过生成模型整合逻辑：
    “2024 年中国新能源汽车销量增速（25%）低于 2023 年（35%），主要原因有两点：1. 政策因素：2024 年新能源汽车购置补贴全面退坡，部分消费者购车意愿回落；2. 基数效应：2023 年因购置税优惠政策，提前释放了大量购车需求，导致 2024 年的增长基数较高，增速相对放缓。”
    不仅给出答案，还能梳理因果关系，体现语义理解能力。

  ## 重点摘 - 智能体Agent

  **从软件工程的角度来看，大模型Agent/智能体，是指基于大语言模型的，能使用工具与外部世界进行交互的计算机程序。**

  如果把Agent=人类，那么大模型=大脑，而工具=四肢。Agent能够通过工具实现与外部世界的交互，而工具通常就是之前介绍过的插件。

  ### 1.2 让Agent具备记忆能力

  #### 1.2.1 短期记忆（Short-Term Memory）

  在大模型Agent中，短期记忆对应着用户与大模型Agent之间的历史对话、提示词、搜索工具反馈的语料块等等，但这些信息仅与当前用户与大模型的对话内容有关。一旦用户关闭应用或离开聊天环境，这类信息就消失了。

  ### 1.2.2 长期记忆（Long-Term Memory）

  在大模型Agent中，长期记忆对应着系统持久化的信息，如业务历史记录、知识库等，通常存储在外部向量数据库和文档库中，Agent会利用长期记忆来回答用户私有知识或专业领域相关的问题。

### 	1.3 让Agent具备规划能力

我们可以让Agent在处理复杂任务时，通过将大任务拆分成小任务，可以更有条理地完成整个计划。

### **1.3.1 任务分解**

任务分解的方式，即思维链、思维树等方式，这里将针对Agent场景进一步探讨。

![img](https://scms-prod-sh-public.oss-cn-shanghai.aliyuncs.com/course_picture/erqnqkqkwqudcgphbhcf.png)

#### 1.3.1.1 思维链（Chain of Thought，CoT）

把复杂任务分解为链式思维最早是用来解决数学题的。

#### **1.3.1.2 思维树ToT（Tree of Thought）**

思维树（ToT）则进一步扩展了思维链的概念，通过构建一个具有分支和选择的树形结构来处理复杂问题。

 在这一模式中，我们尽可能让大模型生成多条可以探索尝试的路径，我们在应用层面评估每条路径的结果和影响，然后选择最优的解决方案。这种方法特别适用于那些不确定性较高、路径选择众多的问题，例如复杂的决策树分析、多步骤规划，以及需要权衡不同因素的优化问题。

#### **1.3.1.3 思维**图**GoT（Graph of Thoughts）及其他**

虽然ToT的方法比较接近人类思考问题的方式，但在实际使用中，我们还可以考虑对思维树进行执行上的优化，比如[Besta等人（2023）](https://arxiv.org/abs/2308.09687)提出了思维图（GoT）。比如在执行中发现了问题可以回溯到上一步；在评估前对执行出了问题的方案做剪枝（在同一层多个相同执行的节点可以进行合并等），所以我们可以把思维图看做是对思维树的工程优化。

实际应用中我们也要考虑树思维做剪枝，比如系统在同一步骤中生成的多个治疗方案有可能是重复的，而有一些疾病的治疗手段非常相似。
