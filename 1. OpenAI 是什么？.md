## 一、OpenAI

### 1. OpenAI 是什么？

OpenAI 是一家人工智能研究公司，以其开发的 GPT (Generative Pre-trained Transformer) 系列大语言模型而闻名。通过 OpenAI API，开发者可以将这些强大的 AI 能力（如自然语言理解、文本生成、代码编写、语音处理等）集成到自己的 Python 应用程序中。

### 2. 核心概念：模型 (Model)

OpenAI 提供了多种不同能力的模型，你需要根据具体任务选择最合适的一个。

- **入门首选**：`gpt-3.5-turbo`
- **追求极致性能**：`gpt-4o`
- **处理图像**：`gpt-4o`
- **处理语音**：`whisper-1` 和 `tts-1`
- 通义千问大语言模型：
  - 商业版（[通义千问Max](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#d4ccf72f23jh9)、[通义千问Plus](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#5ef284d4ed42p)、[通义千问Flash](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#13ff05e329blt)）、
  - 开源版（[Qwen3](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#94410dcf1al50)、[Qwen2.5](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#15f2bdc5dd3zd)）、超长文档模型[通义千问Long](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#27b2b3a15d5c6)
  - 第三方模型：[DeepSeek](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#935bd5ba5cg5d)、[Kimi](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#2363cbf60fe6m)、[GLM-4.5](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#059d6a5d1chfp)等。

### 3. 核心参数 (必选或最常用)

| 参数名         | 类型         | 描述                                                         | 示例                                                         |
| -------------- | ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **`model`**    | `str`        | **(必需)** 要使用的模型的 ID。                               | `"gpt-3.5-turbo"`, `"gpt-4o"`, `"gpt-4o-mini"`，  商业版（[通义千问Max](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#d4ccf72f23jh9)、[通义千问Plus](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#5ef284d4ed42p)、[通义千问Flash](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#13ff05e329blt)）、 开源版（[Qwen3](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#94410dcf1al50)、[Qwen2.5](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#15f2bdc5dd3zd)）、超长文档模型[通义千问Long](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#27b2b3a15d5c6) 第三方模型：[DeepSeek](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#935bd5ba5cg5d)、[Kimi](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#2363cbf60fe6m)、[GLM-4.5](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#059d6a5d1chfp)等。 |
| **`messages`** | `list[dict]` | **(必需)** 对话的上下文列表。每个元素是一个字典，包含 `role` 和 `content`。 | `[{"role": "user", "content": "你好"}]`                      |



### **3.1 model**

#### (1) OpenAI: `"gpt-3.5-turbo"`, `"gpt-4o"`, `"gpt-4o-mini"`，  

#### (2) 通千问：

​            商业版（[qwen-max](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#d4ccf72f23jh9)、[qwen-plus](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#5ef284d4ed42p)、[通义千问Flash](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#13ff05e329blt)）、 

​             开源版（[Qwen3](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#94410dcf1al50)、[Qwen2.5](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#15f2bdc5dd3zd)）、超长文档模型[qwen-long](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#27b2b3a15d5c6) 

#### (3)第三方模型：[DeepSeek](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#935bd5ba5cg5d)、[Kimi](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#2363cbf60fe6m)、[GLM-4.5](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.i1#059d6a5d1chfp)等。

### **3.2.  messages 的基本结构**

`messages` 是一个由字典（`dict`）组成的列表。每个字典代表对话中的一条消息，并且必须包含以下两个键：

#### **(1) role : 消息的角色**

`role` 决定了消息在对话中的角色，模型会据此调整其行为。

 `role` 的三种类型 

##### 1). "system"` (系统消息)

- **作用**: **设定助手的行为、性格和规则**。它像一个 “幕后导演”，在整个对话开始前给模型下达指令。
- 特点:
  - 它**不会**作为对话的一部分被用户看到。
  - 它会影响模型生成所有后续回答的风格和内容。
  - 通常只在对话开始时提供一次，但也可以在多轮对话中动态更新。
- 示例:
  - `"你是一个友好且有耐心的助手。"`
  - `"请用简洁明了的语言回答，避免使用专业术语。"`
  - `"你是一个翻译专家，请将用户的问题从中文翻译成英文。"`

##### 2). `"user"` (用户消息)

- **作用**: **代表用户的输入或提问**。
- 特点:
  - 这是模型需要直接回应的内容。
  - 在多轮对话中，你需要将用户的每一次新提问都作为一个新的 `"user"` 消息添加到 `messages` 列表中。

##### 3) .`"assistant"` (助手消息)

- **作用**: **代表模型（助手）之前的回答**。
- 特点:
  - 这是实现 ** 多轮对话（上下文记忆）** 的核心。
  - 当进行多轮对话时，你**必须**将模型上一次的回答作为 `"assistant"` 消息添加回 `messages` 列表中。这样，模型才能 “记住” 之前说过的话，并在此基础上进行新的回答。

#### (2)**`content`**: 消息的内容。

```python
messages = [
    {"role": "system", "content": "你是一个专业的Python编程助手。"},
    {"role": "user", "content": "解释一下什么是列表推导式？"},
    {"role": "assistant", "content": "列表推导式是Python中创建列表的一种简洁语法..."},
    {"role": "user", "content": "用它写一个例子，筛选出1到100中的偶数。"}
]
```

##### 1）**content：纯文本字符串**

##### 2）**content：多模态输入**列表

从 GPT-4o 模型开始，`content` 不仅可以是纯文本字符串，还可以是一个**列表**，从而支持**文本和图像**的混合输入。

**结构**:

当 `content` 是一个列表时，列表中的每个元素是一个字典，必须包含 `type` 和 `text` 或 `image_url`。

- `{"type": "text", "text": "你的文字描述"}`
- `{"type": "image_url", "image_url": {"url": "图片的URL"}}`

**示例：询问图片内容**

```python
from openai import OpenAI

client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "描述一下这张图片里有什么，并用中文回答。"},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Example_image.svg/1200px-Example_image.svg.png",
                    },
                },
            ],
        }
    ],
)

print(response.choices[0].message.content)
```

#### **3.3.  temperture**

`temperature` 是控制大语言模型生成文本**随机性**和**创造性**的最核心参数。`temperature` 通过一个数学公式（称为 “**softmax 温度缩放**”）来调整每个词的概率。

##### (1)核心概念：什么是 “**候选 Token 集合的概率分布**”？

在模型生成每个词（Token）之前，它不会只思考一个词。相反，它会为词汇表中**所有可能的下一个词**计算一个**概率分数**。这个分数代表了模型认为哪个词最适合放在当前位置。

我们可以把这个过程想象成一个**抽奖转盘**：

- **词汇表中的每个词** 都是转盘上的一个区域。
- **概率分数** 决定了每个区域的**大小**。概率越高，区域越大，被选中的可能性就越大。

**例如**，假设模型在句子 "我想去____" 之后，计算出的下一个词的概率分布如下：

| 候选词 (Token)       | 概率分数 | 在转盘上的区域大小 |
| -------------------- | -------- | ------------------ |
| **公园**             | 30%      | 最大的一块         |
| **学校**             | 25%      | 第二大块           |
| **商店**             | 20%      | 第三大块           |
| **电影院**           | 15%      | 中等大小           |
| **月球**             | 5%       | 很小的一块         |
| **... (其他所有词)** | 5%       | 剩余的微小区域     |

在这个例子中，“公园” 是最可能被选中的词，但模型也有可能选择 “学校”、“商店”，甚至是概率很低的 “月球”。

`temperature` 参数就是用来 **“揉捏” 这个概率分布（抽奖转盘）的形状 **。

##### (2). `temperature` 如何影响概率分布？

`temperature` 通过一个数学公式（称为 “**softmax 温度缩放**”）来调整每个词的概率。这个公式的效果可以总结为：

- **低 `temperature` (< 1)**：会**放大**高概率词的优势，同时**压缩**低概率词的机会。它让概率分布变得更 “尖锐”。

- **高 `temperature` (> 1)**：会**拉平**概率分布，让低概率的词有更大的机会被选中。它让概率分布变得更 “平坦”。

- **`temperature` = 1**：保持模型原始的概率分布不变。

**⚠️ 重要提示**：`temperature` 的有效范围通常是 `0` 到 `2`。设置为 `0` 是一个特殊情况，它会导致模型总是选择概率最高的那个词（确定性模式）。

##### (3). 实例演示：不同 `temperature` 值的效果

让我们用上面的 “我想去____” 例子，看看不同 `temperature` 值如何改变抽奖转盘。

- 场景一：低 Temperature (e.g., `temperature = 0.2`)

低温度会让概率分布变得非常 “尖锐”。高概率的词（如 “公园”）的区域会变得极大，而低概率的词（如 “月球”）的区域会变得几乎看不见。

**调整后的概率分布（示意）：**

| 候选词 (Token) | 原始概率 | 调整后概率 (低 temp) | 转盘变化     |
| -------------- | -------- | -------------------- | ------------ |
| **公园**       | 30%      | **85%**              | 区域变得巨大 |
| **学校**       | 25%      | **10%**              | 区域变小     |
| **商店**       | 20%      | **4%**               | 区域变得很小 |
| **电影院**     | 15%      | **1%**               | 区域几乎消失 |
| **月球**       | 5%       | **≈ 0%**             | 区域消失     |

**效果**：模型的选择变得非常**保守和可预测**。它几乎总是会选择 “公园”。生成的文本会非常**安全、稳定、重复性高**，但可能会显得**枯燥、缺乏创意**。

**适用场景**：

- 需要事实性、准确性的任务，如问答、代码生成、数据整理。
- 例如：“请解释什么是光合作用。” 你希望得到一个标准、准确的解释，而不是一个富有诗意的比喻。

- 场景二：高 Temperature (e.g., `temperature = 1.8`)

高温度会让概率分布变得非常 “平坦”。原本概率低的词的区域会被显著扩大。

**调整后的概率分布（示意）：**

| 候选词 (Token) | 原始概率 | 调整后概率 (高 temp) | 转盘变化       |
| -------------- | -------- | -------------------- | -------------- |
| **公园**       | 30%      | **25%**              | 区域变小       |
| **学校**       | 25%      | **23%**              | 区域略微变小   |
| **商店**       | 20%      | **20%**              | 区域基本不变   |
| **电影院**     | 15%      | **18%**              | 区域变大       |
| **月球**       | 5%       | **14%**              | 区域显著变大！ |

**效果**：模型的选择变得非常**随机和有创造性**。它有相当大的概率会选择 “月球” 这种出人意料的词。生成的文本会更**多样化、有想象力**，但也更容易出现**胡言乱语、事实性错误**。

**适用场景**：

- 需要创造性的任务，如头脑风暴、写诗、写故事、角色对话。
- 例如：“给我一个关于机器人和猫成为朋友的短故事创意。” 你希望得到新颖、有趣的想法。

------

- 总结

| `temperature` 值 | 效果       | 生成文本特点                           | 适用场景                       |
| ---------------- | ---------- | -------------------------------------- | ------------------------------ |
| **接近 0**       | **确定性** | 非常保守、可预测、重复性高、事实准确   | 问答、代码、数据处理、事实总结 |
| **0.5 - 1.0**    | **平衡**   | 合理、自然、有一定创造性               | 大多数日常对话、内容创作       |
| **1.5 - 2.0**    | **随机性** | 高度创造性、多样化、可能出现幻觉或错误 | 头脑风暴、诗歌、故事、创意写作 |

#### 3.4 top_p-核采样

##### (1). 核心概念：什么是 `top_p`？

`top_p`（也称为 Nucleus Sampling，核采样）是一种**动态截断**概率分布的方法。它的工作原理如下：

1. 模型为下一个词计算出所有可能候选词的概率分布。
2. 它将这些候选词按概率从高到低**排序**。
3. 然后，它从概率最高的词开始，依次将它们的概率相加。
4. 当累积概率达到你设定的 `top_p` 值时，它就**停止**。
5. 模型**只在这个累积概率达到 `top_p` 的 “核心”（Nucleus）词集合中进行选择**。所有其他的词都被丢弃。

我们可以把这个过程想象成一个 **“抓阄”** 游戏：

- 你有一个大袋子，里面装满了写有不同词语的小纸条，每个词语的数量与其概率成正比。
- **`top_p = 0.9` (90%)** 的意思是：你从袋子里拿出纸条，先拿出概率最高的，再拿第二高的... 直到你手中纸条的总 “权重” 达到了 90%。然后，你把剩下的 10% 权重的纸条全部扔掉，**只在你手中的这 90% 的纸条里**随机抽一个。

**关键点**：`top_p` 关注的是**概率的累积和**，而不是某个词的绝对概率。它的 “候选词集合” 大小是**动态变化**的。

##### (2). `top_p` 如何影响候选词集合？

`top_p` 的值决定了这个 “核心” 集合的大小。

- **高 `top_p` (e.g., `0.95`)**：需要包含更多的词才能让累积概率达到 95%。这会导致一个**更大、更多样化**的候选词集合。
- **低 `top_p` (e.g., `0.1`)**：只需要一两个概率最高的词就能让累积概率达到 10%。这会导致一个**非常小、高度集中**的候选词集合。

##### (3). 实例演示：不同 `top_p` 值的效果

让我们用之前的例子：模型在句子 "我想去____" 之后，计算出的下一个词的概率分布如下：

| 候选词 (Token)       | 概率分数 |
| -------------------- | -------- |
| **公园**             | 30%      |
| **学校**             | 25%      |
| **商店**             | 20%      |
| **电影院**           | 15%      |
| **月球**             | 5%       |
| **海滩**             | 3%       |
| **... (其他所有词)** | 2%       |

- 场景一：高 `top_p` (e.g., `top_p = 0.9`)

我们需要累积概率达到 90%。

1. 加入 "公园" (30%) -> 累积: 30%
2. 加入 "学校" (25%) -> 累积: 55%
3. 加入 "商店" (20%) -> 累积: 75%
4. 加入 "电影院" (15%) -> 累积: 90% ✅ 达到目标！

**最终候选词集合**：`{"公园", "学校", "商店", "电影院"}`

**被丢弃的词**："月球", "海滩", ...

**效果**：模型有一个相对较大的选择范围，有机会产生一些意想不到但又相对合理的输出（比如 “电影院”）。这通常能在保持输出质量的同时，提供不错的多样性。

- 场景二：低 `top_p` (e.g., `top_p = 0.35`)

我们只需要累积概率达到 35%。

1. 加入 "公园" (30%) -> 累积: 30%
2. 加入 "学校" (25%) -> 累积: 55% ✅ 已经超过 35%，停止！

**最终候选词集合**：`{"公园", "学校"}`

**被丢弃的词**："商店", "电影院", "月球", ...

**效果**：模型的选择范围被极大地限制了。它几乎肯定会在 “公园” 和 “学校” 之间做选择，输出会非常**保守和可预测**。

### 5. top_p,temperature 如何选择和使用？

OpenAI 官方和许多实践经验都给出了明确的建议：

**最佳实践：使用其中一个，不要同时使用两个。**

- **推荐做法**：**设置 `top_p = 0.9` 作为默认值，然后只通过调整 `temperature` 来控制创造性。**
  - `top_p = 0.9` 是一个非常稳健的设置，它能有效过滤掉那些极不可能、通常是错误或无意义的词，同时保留了足够的多样性。
  - 然后，你可以像之前学到的那样，用 `temperature` 来微调随机性。
- **什么时候单独使用 `top_p`？**
  - 当你发现即使在低 `temperature` 下，模型仍然偶尔会产生一些非常奇怪或不相关的回答时，可以尝试降低 `top_p`（例如降到 `0.8` 或 `0.7`）。这相当于给模型的 “想象力” 戴上了一个更紧的 “缰绳”。
- **为什么不建议同时使用？**
  - 同时调整 `temperature` 和 `top_p` 会使结果变得难以预测和调试。你很难判断输出的变化是由哪个参数引起的。通常，单独使用一个参数就能达到很好的效果。

### 6. 多轮对话的工作流程

理解了 `role` 的三种类型后，多轮对话的逻辑就非常清晰了：

1. **初始化**: 创建一个 `messages` 列表，通常以一个 `system` 消息开始。
2. 第一轮:
   - 向 `messages` 列表中添加一个 `user` 消息。
   - 调用 `client.chat.completions.create` 并传入 `messages`。
   - 从 API 响应中获取模型的回答。
   - 将模型的回答作为一个 `assistant` 消息，追加到 `messages` 列表中。
3. 第二轮及后续:
   - 重复上述过程：添加新的 `user` 消息 -> 调用 API -> 获取回答 -> 添加 `assistant` 消息到列表。

**代码示例:**

```python
from openai import OpenAI
client = OpenAI()
# 1. 初始化 messages 列表
messages = [
    {"role": "system", "content": "你是一个知识渊博的历史老师。"}
]
# 2. 第一轮对话
user_input_1 = "告诉我一些关于罗马帝国的有趣事实。"
messages.append({"role": "user", "content": user_input_1})
response_1 = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=messages
)
assistant_reply_1 = response_1.choices[0].message.content
print(f"AI: {assistant_reply_1}")
# 将 AI 的回复添加到对话历史中
messages.append({"role": "assistant", "content": assistant_reply_1})
# --- messages 列表现在包含: [system, user, assistant] ---
# 3. 第二轮对话 (模型会记住第一轮的内容)
user_input_2 = "它最终是如何灭亡的？"
messages.append({"role": "user", "content": user_input_2})

response_2 = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=messages # 传入包含完整历史的 messages 列表
)

assistant_reply_2 = response_2.choices[0].message.content
print(f"AI: {assistant_reply_2}")

# --- messages 列表现在包含: [system, user, assistant, user] ---
# (在实际应用中，你会继续将 assistant_reply_2 添加回去)
```

- 

### 可选参数 (用于控制生成和响应)

#### 1. 控制响应内容和质量

| 参数名                  | 类型                 | 描述                                                         | 示例                                     |           |           |
| ----------------------- | -------------------- | ------------------------------------------------------------ | ---------------------------------------- | --------- | --------- |
| **`temperature`**       | `float`              | **采样温度**。控制响应的随机性。值越高，回答越随机、有创造性；值越低，回答越确定、保守。**范围：0 到 2**。 | `0.7` (平衡), `0` (精确), `1.5` (有创意) |           |           |
| **`top_p`**             | `float`              | **核采样**。模型会从概率最高的 `top_p` 部分词汇中进行选择。这是另一种控制随机性的方法，通常**不要与 `temperature` 同时修改**。**范围：0 到 1**。 | `0.9`                                    |           |           |
| **`n`**                 | `int`                | 为每个输入生成的**响应数量**。默认值为 `1`。                 | `3` (获取 3 个不同的回答)                |           |           |
| **`stream`**            | `bool`               | 如果设为 `True`，模型会以**流式**方式逐块返回数据，而不是等待整个响应生成完毕。这对于实现打字机效果的聊天界面至关重要。 | `True`                                   |           |           |
| **`stop`**              | `str` 或 `list[str]` | 一个或多个序列。当模型生成这些序列中的任何一个时，它会停止生成更多内容。 | `"END"`, `["\n", "。"]`                  |           |           |
| **`max_tokens`**        | `int`                | 模型在响应中可以生成的**最大 token 数量**。注意，这个数量是共享的，包括提示词（`messages`）和生成的回答。如果超出，模型会提前停止。 | `1024`                                   |           |           |
| **`presence_penalty`**  | `float`              | **存在惩罚**。鼓励模型讨论新话题。正值会增加模型使用新 token 的可能性（那些在 prompt 中不常出现的 token）。**范围：-2.0 到 2.0**。 | `0.5`                                    |           |           |
| **`frequency_penalty`** | `float`              | **频率惩罚**。减少模型重复自身的可能性。正值会降低模型重复使用相同 token 的可能性。**范围：-2.0 到 2.0**。 | `0.5`                                    |           |           |
| **`logit_bias`**        | `dict`               | 修改指定 token 出现在完成内容中的可能性。字典的键是 token ID，值是一个介于 -100 到 100 之间的整数。 | `{50256: -100}` (几乎禁止使用 `<         | endoftext | >` token) |
| **`user`**              | `str`                | 一个唯一的标识符，代表你的终端用户。这可以帮助 OpenAI 监控和检测滥用行为。 | `"user_12345"`                           |           |           |



#### temperature：调整候选Token集合的概率分布:

`temperature` 是控制大语言模型生成文本**随机性**和**创造性**的最核心参数。理解它的工作原理，能让你更好地控制模型的输出风格。

------

### 7.`response` 对象

`response` 对象是一个 `openai.types.chat.chat_completion.ChatCompletion` 类型的对象，它包含了丰富的属性。

#### (1) 整体结构概览

一个典型的 `response` 对象结构如下（以 JSON 形式展示，便于理解）：

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1677858242,
  "model": "gpt-3.5-turbo-0613",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "你好！我是一个由 OpenAI 训练的语言模型。请问有什么我可以帮助你的吗？",
        "tool_calls": null,
        "function_call": null
      },
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 13,
    "completion_tokens": 27,
    "total_tokens": 40
  },
  "system_fingerprint": "fp_abc123"
}
```

#### (2). 核心属性详解

#### `id`

- **类型**: `str`
- **描述**: 本次请求的唯一标识符。可以用于追踪特定的 API 调用。

#### `object`

- **类型**: `str`
- **描述**: 对象类型，固定为 `"chat.completion"`。用于区分不同类型的 API 响应（例如，与 `"text_completion"` 区分）。

#### `created`

- **类型**: `int`
- **描述**: 响应创建的时间戳，单位是 Unix 时间（从 1970-01-01 00:00:00 UTC 到现在的秒数）。

#### `model`

- **类型**: `str`
- **描述**: 本次请求实际使用的模型 ID。这可能与你请求的模型略有不同（例如，你请求 `gpt-3.5-turbo`，实际可能使用 `gpt-3.5-turbo-0613`）。

#### **`choices` (最重要的部分)**

- **类型**: `list[ChatCompletionChoice]`
- **描述**: 一个列表，包含了模型生成的一个或多个回答选项。因为你可以通过 `n` 参数请求多个回答，所以这是一个列表。通常我们只取第一个元素 `choices[0]`。

**每个 `choice` 对象包含以下属性：**

- **`index`**: `int`。在 choices 列表中的索引。

- `message`:

  ```
  ChatCompletionMessage
  ```

  。这是最重要的部分，包含了模型生成的消息。

  - **`role`**: `str`。通常是 `"assistant"`。
  - **`content`**: `str` 或 `None`。模型生成的文本内容就在这里。这是你最常访问的字段。
  - **`tool_calls`**: `list[ChatCompletionMessageToolCall]` 或 `None`。当你使用函数调用（Function Calling）时，这里会包含模型请求调用的工具信息。
  - **`function_call`**: `ChatCompletionMessageFunctionCall` 或 `None`。这是 `tool_calls` 的旧版形式，现在已不推荐使用。

- **`logprobs`**: `ChatCompletionLogprobs` 或 `None`。如果在请求时设置了 `logprobs=True`，这里会包含每个生成 token 的概率信息，用于分析模型的决策过程。

- `finish_reason`:

  ```
  str
  ```

  。表示模型停止生成文本的原因。

  - `"stop"`: 模型遇到了自然的停止点（如句尾）或你指定的 `stop` 序列。
  - `"length"`: 模型生成的 token 数量达到了 `max_tokens` 的限制。
  - `"tool_calls"`: 模型停止是因为它决定调用一个工具（函数）。
  - `"content_filter"`: 由于内容政策限制，模型停止了生成。

#### `usage`

- **类型**: `ChatCompletionUsage`

- 描述

  : 本次请求的 token 使用情况，这直接关系到你的 API 费用。

  - **`prompt_tokens`**: `int`。你的输入（`messages`）消耗的 token 数量。
  - **`completion_tokens`**: `int`。模型生成的输出消耗的 token 数量。
  - **`total_tokens`**: `int`。总消耗的 token 数量 (`prompt_tokens + completion_tokens`)。

#### `system_fingerprint`

- **类型**: `str`
- **描述**: 一个哈希值，代表本次请求所使用的模型配置（包括模型版本、训练数据等）。当你需要复现结果时，这个字段很有用。

####  (3). 如何在 Python 中访问这些属性

```python
from openai import OpenAI
client = OpenAI()
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "你好！"}]
)

# 1. 获取模型生成的文本内容
message_content = response.choices[0].message.content
print(f"AI 的回答: {message_content}")

# 2. 获取 token 使用情况
prompt_tokens = response.usage.prompt_tokens
completion_tokens = response.usage.completion_tokens
total_tokens = response.usage.total_tokens
print(f"消耗的 Token: 输入={prompt_tokens}, 输出={completion_tokens}, 总计={total_tokens}")

# 3. 获取其他元数据
request_id = response.id
model_used = response.model
finish_reason = response.choices[0].finish_reason

print(f"请求 ID: {request_id}")
print(f"使用的模型: {model_used}")
print(f"停止原因: {finish_reason}")
```

#### (4). 流式输出 (Streaming)

当你在请求中设置 `stream=True` 时，`response` 对象不再是一个单一的 `ChatCompletion` 对象，而是一个**迭代器 (iterator)**。

这个迭代器会逐块（chunk）地返回数据。每一块的结构与完整的 `response` 类似，但只包含增量的信息。

**流式 `chunk` 的结构：

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion.chunk",
  "created": 1677858242,
  "model": "gpt-3.5-turbo-0613",
  "choices": [
    {
      "index": 0,
      "delta": {
        "role": "assistant", // 只在第一个 chunk 中出现
        "content": "你"     // 增量的文本内容
      },
      "finish_reason": null // 在最后一个 chunk 中才会有值
    }
  ],
  "system_fingerprint": "fp_abc123"
}
```

**关键点：**

- **`object`**: 类型变为 `"chat.completion.chunk"`。
- **`choices[0].delta`**: 包含了**增量**的变化。你需要将所有 `chunk` 的 `delta.content` 拼接起来，才能得到完整的回答。
- **`usage`**: 在流式模式下，**不会**在每个 `chunk` 中返回。只有当流结束后，你才会收到一个包含 `usage` 信息的最终 `chunk`。

**流式处理代码示例：**

```python
from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "请用100字介绍一下你自己。"}],
    stream=True,
)

print("AI 的回答: ", end="", flush=True)
full_response = ""
for chunk in stream:
    # 检查 delta 中是否有 content
    if chunk.choices[0].delta.content is not None:
        content = chunk.choices[0].delta.content
        full_response += content
        print(content, end="", flush=True)

print() # 换行
print(f"\n完整的回答: {full_response}")
```

#### (5)总结

- **非流式响应**：你得到一个完整的 `ChatCompletion` 对象，直接通过 `response.choices[0].message.content` 获取回答，并通过 `response.usage` 查看 token 消耗。
- **流式响应**：你得到一个迭代器，需要循环它，通过 `chunk.choices[0].delta.content` 拼接出完整的回答。

熟练掌握这两种响应结构的处理方式，是高效使用 OpenAI API 的基础。

### 8.模型的工作流程

我们把模型的工作流程分解成两个主要阶段：

#### 8.1 阶段一：编码阶段 (Encoding / Forward Pass)

1. **输入**：你提供的 `messages` (即整个对话历史)。
2. **过程**：模型读取你的输入，并在其内部的神经网络中进行一系列复杂的计算。这个过程会将你的输入转换成一个高维的数学表示（通常称为 “上下文向量” 或 “隐藏状态”）。
3. **输出**：这个阶段的**输出**是一个关于**下一个词（Token）\**的\**原始概率分布**。这个分布包含了词汇表中**每一个词**被选为下一个词的可能性分数。

**关键点**：在这个阶段，`temperature` 和 `top_p` **还没有发挥任何作用**。模型只是根据你的输入，忠实地计算出了它认为最 “原始” 的可能性。

#### 8.2 阶段二：生成阶段 (Decoding / Sampling)

1. **输入**：上一阶段计算出的**原始概率分布**。
2. 过程：这正是temperature和top_p大显身手的地方。它们作为采样策略 (Sampling Strategies)，决定了如何从这个概率分布中挑选出最终要输出的那个词。
   - **`temperature`**：会 “揉捏” 这个原始概率分布，让它变得更尖锐（低温度）或更平坦（高温度）。
   - **`top_p`**：会根据概率总和，动态地截断这个分布，只在一个 “核心” 的候选词集合中进行选择。
3. **输出**：一个被选中的词（Token）。

#### 8.3 循环往复的过程

生成阶段并不是一次性完成的。模型是**逐词生成**的：

1. 它根据你最初的输入，生成第一个词。
2. 然后，它把**这个生成的词**追加到你的输入中，形成一个新的、更长的上下文。
3. 模型再次进入**编码阶段**，计算这个新上下文的概率分布。
4. 再次进入**生成阶段**，使用同样的 `temperature` 和 `top_p` 设置，从新的概率分布中选择第二个词。
5. 这个 “编码 -> 生成 -> 追加” 的循环会一直重复，直到模型生成了结束符（如 `<|end_of_solution|>`）或达到了 `max_tokens` 的限制。

## 二、RAG

### 1.大模型不能回答私域知识问题

根本原因在于，大模型的知识来源于其训练数据，这些数据通常是公开的互联网信息，**不包含**任何特定公司的内部文档、政策或流程。

#### 1.1 初步解决方案：在提示词中“喂”入知识

将公司项目管理工具的说明文档，**直接添加到给模型的指令（System Prompt）中**，作为背景知识提供给它。

##### （1） 核心瓶颈：有限的上下文窗口

大模型接收我们输入（包括指令、问题和背景知识）的地方，被称为**上下文窗口（Context Window）**。它的容量是有限的。 输入内容超过模型的最大限制：将整个公司的知识库（成百上千份文档）一次性塞进这个有限的窗口，输入内容超过模型的最大限制，就会导致错误。

其次：

1. **效率低**：上下文越长，大模型处理所需的时间就越长，导致用户等待时间增加。
2. **成本高**：大部分模型是按输入和输出的文本量计费的，冗长的上下文意味着更高的成本。
3. **信息干扰**：如果上下文中包含了大量与当前问题无关的信息，就像在开卷考试时给了考生一本错误科目的教科书，反而会干扰模型的判断，导致回答质量下降。

##### （2）解决之道：上下文工程 (Context Engineering)

如何**在正确的时间**，将**最相关**、**最精准**的知识，**动态地**加载到大模型有限的**上下文窗口中**？——这门系统性地设计、构建和优化上下文的实践，就是**上下文工程（Context Engineering）**。

从这个角度看，**许多大模型应用的失败，并非模型本身不够智能，而是“上下文”的失败**。上下文工程正是释放大模型潜力的关键所在。

### 2.上下文工程 (Context Engineering) 的核心技术

主要包括：

- **RAG (检索增强生成)**：从外部知识库（如公司文档）中**检索**信息，为模型提供精准的回答依据。
- **Prompt (提示词工程)**：通过精心设计的**指令**，精确地引导模型的思考方式和输出格式。
- **Tool (工具使用)**：赋予模型调用外部**工具**（如计算器、搜索引擎、API）的能力，以获取实时信息或执行特定任务。
- **Memory (记忆机制)**：为模型建立长短期**记忆**，使其能够在连续对话中理解历史上下文。

### 3.**RAG (检索增强生成)**

**RAG（Retrieval-Augmented Generation，检索增强生成）** 就是实现上下文工程的强大技术方案。

- 核心思想是：

在用户提问时，不再将全部知识库硬塞给大模型，而是先**自动检索**出与问题最相关的私有知识片段，然后将这些精准的片段与用户问题合并后，一同传给大模型，从而**生成**最终的答案。这样既避免了提示词过长的问题，又能确保大模型获得相关的背景信息。

- ### 1. 没有 RAG 的 “痛点”：模型的 “幻觉”

  - ### RAG 的核心价值

    | 特性         | 没有 RAG 的 LLM                               | 有 RAG 的 LLM                                                |
    | ------------ | --------------------------------------------- | ------------------------------------------------------------ |
    | **知识来源** | 模型在训练时学到的通用知识。                  | **你的私有知识库 + 模型的通用知识**。                        |
    | **回答依据** | 猜测、推理。                                  | **有据可查**，基于检索到的文本片段。                         |
    | **准确性**   | 对最新、最具体的信息，准确性低，容易 “幻觉”。 | **准确性高**，因为答案锚定在事实资料上。                     |
    | **知识更新** | 几乎不可能，除非重新训练模型。                | **非常容易**，只需更新你的知识库即可。                       |
    | **应用场景** | 通用问答、头脑风暴、创意写作。                | **企业问答、客服、数据分析、知识管理**等需要精确信息的场景。 |

总的来说，基于 RAG 结构的应用，既避免了将整个参考文档作为背景信息输入而导致的各种问题，又通过检索提取出了与问题最相关的部分，从而提高了大模型输出的准确性与相关性。解决大模型幻觉。

### 4.RAG的工作原理

RAG应用通常包含**建立索引**与**检索生成**两部分。

#### 4.1 建立索引

建立索引包括四个步骤：

1. **文档解析**
   RAG应用也需要首先将知识库文档进行加载并解析为大模型能够理解的文字形式。

   - 第 1 步：加载数据 (Data Loading)

   **目标**：将 PDF 文件读入程序，并转换成 LlamaIndex 可以处理的`Document`对象。

   ```mermaid
   sequenceDiagram
   PDF文件-->>Document: DataConnector SimpleDirectoryReader.load_data()
   Document-->>node(chunk):VectorStoreIndex.from_documents()-TokenSplitter
   node(chunk)-->>Index:VectorStoreIndex.from_documents()-Embending 
   
   ```

   

    **LlamaIndex Data Connector 部件**：`SimpleDirectoryReader 

   ```python
   from llama_index.core import SimpleDirectoryReader
   print("正在解析文件...")
   # LlamaIndex提供了SimpleDirectoryReader方法，可以直接将指定文件夹中的文件加载为document对象，对应着解析过程
   documents = SimpleDirectoryReader('./docs').load_data()
   ```

   

2. **文本分段**
   RAG应用也会在文档解析后对文本进行分段，以便于在后续能够快速找到与提问最相关的内容。

   #### 第 2 步：文档切分 (Document Chunking)

   **目标**：将长文档切分成小块（`Node`），因为 LLM 的上下文窗口有限，且小块内容的检索精度更高。

   **LlamaIndex 部件**：**VectorStoreIndex**  **TokenSplitter** (默认的 Text Splitter)

   **工作细节**：当你使用`VectorStoreIndex.from_documents()`时，LlamaIndex 会自动调用切分器。它会根据 token 数量（默认约 512 个 token）来切分文档。

   **切分后**：原来的 1 个`Document`对象，被切分成了多个`Node`对象。每个`Node`都包含一部分文本和其在原始文档中的位置信息。

3. **文本向量化**
   RAG应用中，通常需要借助嵌入（embedding）模型分别对段落与问题进行数字化表示，在进行相似度比较后找出最相关的段落，数字化表示的过程就叫做文本向量化。

   

   #### 第 3 步：构建索引 (Index Construction)

   **目标**：将切分好的`Node`对象，通过 Embedding 模型转换成向量，并存储在向量数据库中，构建一个可高效检索的索引。

   **LlamaIndex 部件**：**VectorStoreIndex**, `Embedding`模型（比如，阿里云**DashScopeEmbedding**）

   ```python
   print("正在完成切片并创建索引...")
   index = VectorStoreIndex.from_documents(
       documents= documents,
       embedding = DashScopeEmbedding(
           model_name = DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V3,
           embed_batch_size=6,
           embed_input_length=8192
       )
   ```

   

4. **存储索引**
   存储索引将向量化后的段落存储为向量数据库.

   创建索引消耗的时间比较长。如果能够将索引保存到本地，并在需要使用的时候直接加载，而不是重新建立索引，那就可以大幅提升回复的速度，LlamaIndex提供了简单易实现的保存与加载索引的方法.

    **构建索引结构**：向量存储内部会建立一个索引（如 HNSW），以便后续能快速进行相似度搜索。

   

   ```python
   # 将索引保存为本地文件
   index.storage_context.persist("knowledge_base/test")
   print("索引文件保存到了knowledge_base/test")
   ```

**工作细节**：

1. **获取`Node`**：接收上一步切分好的所有`Node`。
2. **生成向量**：LlamaIndex 会遍历每个`Node`，将其文本内容输入到你配置的`OpenAIEmbedding`模型中。模型会返回一个高维向量（例如 1536 维），这个向量在数学上代表了文本的语义。
3. **存储向量**：这些`Node`向量和它们对应的原始文本会被存储起来。默认情况下，LlamaIndex 使用一个**内存中的向量存储**。在生产环境中，你会将其替换为专门的向量数据库，如**Chroma, Pinecone, Milvus**等。
4. **构建索引结构**：向量存储内部会建立一个索引（如 HNSW），以便后续能快速进行相似度搜索。
   - Index 是 LlamaIndex 的核心。它将`Node`对象进行处理（主要是计算 Embedding 向量），并组织起来以便快速检索。
   - 最核心类型：
     - `VectorStoreIndex`：这是最常用的索引。它将每个`Node`转换为向量（Embedding），并存储在向量数据库中。当查询时，它会计算查询的向量，并找出最相似的`Node`。这是标准 RAG 流程的基础。
   - 其他高级类型：
     - `SummaryIndex`：为文档创建摘要，适合做概览性问答。
     - `TreeIndex`：构建树状结构，适合需要多步推理或总结长文档的场景。
     - `KeywordTableIndex`：基于**关键词进行检索**，适合对精确关键词敏感的场景。

#### 4.2 **检索**阶段   - Retrievers (检索器)

- **作用**：根据查询从 Index 中找回最相关的`Node`。

  Retriever 是连接 “索引” 和 “生成” 的桥梁。你可以从一个`Index`中创建一个`Retriever`，然后用它来执行检索。

- 常见类型：

  - `VectorIndexRetriever`：从`VectorStoreIndex`中进行相似度检索。
  - `SummaryIndexRetriever`：从`SummaryIndex`中检索。
  - **高级用法**：可以组合多个 Retriever（如`EnsembleRetriever`），综合不同检索策略的优点，提升召回率。

#### 4.3. Query Engines (查询引擎)

- **作用**：接收自然语言查询，返回最终答案。

- **扮演角色**：“首席咨询师”。

- **详细说明**：这是执行 RAG 完整流程的高级接口。你通常直接与`QueryEngine`交互，而无需手动调用 Retriever 和 LLM。

- 工作流程：

  1. 接收你的问题（如`query_engine.query("什么是LlamaIndex?")`）。
  2. 内部调用`Retriever`，从`Index`中找到最相关的`Node`。
  3. 将问题和检索到的`Node`打包成一个精心设计的`Prompt`。
  4. 将`Prompt`发送给 LLM（如 GPT-4, Llama 3）。
  5. 接收 LLM 的输出，并返回给你。

- **高级功能**：`QueryEngine`可以配置不同的 “响应模式”（如**仅检索**、**仅生成**、**RAG 混合**），并支持复杂的查询分析。

  

##### 4.3.1 **index.as_query_engine**

index.as_query_engine 作用是从一个已经构建好的 `Index`（索引）中创建一个 `QueryEngine`（查询引擎）。这个 `QueryEngine` 是你与知识库进行交互的直接接口，你向它提问，它返回答案。

- 核心概念：`QueryEngine` 的内部构成

一个标准的 `QueryEngine` 主要包含两个部分：

1. **Retriever (检索器)**：负责根据你的问题，从 `Index` 中找到最相关的文本片段Node。
2. **Response Synthesizer (响应合成器)**：负责将检索到的文本片段（`Node`）和你的原始问题打包成一个 Prompt（提示词），然后发送给大语言模型（LLM）来生成最终的回答。

`as_query_engine()` 的大部分参数，都是用来配置这两个内部组件的。

- as_query_engine() 关键参数详解

 1. 配置检索（Retrieval）行为的参数

    这些参数直接影响 “找什么” 和 “怎么找” 的问题。

    -  **retriever** 类型：BaseRetriever

      **作用**：（高级）直接指定一个自定义的检索器。如果你需要使用复杂的检索策略（如组合多个检索器），可以先创建好 `Retriever` 对象，再通过此参数传入。默认情况下，LlamaIndex 会根据 `Index` 的类型自动创建一个默认的 `Retriever`。

      **示例**：

      ```python
      from llama_index.core.retrievers import VectorIndexRetriever
      retriever = VectorIndexRetriever(index=index, similarity_top_k=5)
      query_engine = index.as_query_engine(retriever=retriever)
      ```

      -  **similarity_top_k** 类型：int

      **作用**：设置检索时返回的最相似的 `Node` 数量。

      这是最常用的参数之一，直接影响检索的召回率和精度。

      - **值越大**：召回的信息越全面，但可能混入不相关的信息，增加 LLM 的处理负担，甚至导致 “幻觉”。

      - **值越小**：结果更精准，但可能漏掉重要的上下文。**默认值通常是 2**，

        你需要根据你的文档长度和 LLM 的上下文窗口大小进行调整。

      **示例**：

      ```python
      query_engine = index.as_query_engine(similarity_top_k=5)
      ```

      -  **vector_store_query_mode** 类型：str

      **作用**：指定向量检索的模式。

      这个参数仅对 `VectorStoreIndex` 有效。

      ​	1) `"default"`：标准的相似度搜索。

      ​	2)`"sparse"`：使用稀疏向量检索（如 BM25 算法），对关键词匹配更敏感。

      ​	3) `"hybrid"`：混合检索，结合稠密向量（Embedding）和稀疏向量的优点，通常能获得更好的效果

      **示例**：

      ```python
      query_engine = index.as_query_engine(vector_store_query_mode="hybrid") 
      ```

      - **alpha** 类型：float

        **作用**：当 `vector_store_query_mode="hybrid"` 时，用于平衡稠密和稀疏检索结果的权重。

        - `alpha=0`：完全使用稀疏检索结果。
        - `alpha=1`：完全使用稠密检索结果。
        - `0 < alpha < 1`：混合两者，`alpha`越大，稠密结果权重越高。

        

        **示例**：

        ```python
         query_engine = index.as_query_engine(vector_store_query_mode="hybrid", alpha=0.7)
        ```

        

      2. 配置生成（Response Synthesis）行为的参数

         这些参数影响 “如何基于检索到的信息生成答案” 的问题。

         | 参数名          | 类型              | 作用与详解                                                   | 示例                                                         |
         | --------------- | ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
         | `synthesizer`   | `BaseSynthesizer` | **（高级）直接指定一个自定义的响应合成器**。与 `retriever` 参数类似，如果你需要自定义生成逻辑（如使用不同的 LLM、不同的 Prompt 模板），可以先创建 `Synthesizer` 对象再传入。 | `from llama_index.core.response_synthesizers import TreeSummarize``synthesizer = TreeSummarize(llm=my_custom_llm)``query_engine = index.as_query_engine(synthesizer=synthesizer)` |
         | `response_mode` | `str`             | **指定响应合成的模式**。这是控制生成逻辑的关键参数。- `"compact"` (默认): 将所有检索到的 `Node` 压缩成一个简洁的 Prompt，一次性发送给 LLM。效率高，是默认推荐。- `"refine"`：迭代式地生成答案。先基于第一个 `Node` 生成一个初步答案，然后用第二个 `Node` 来 “优化” 这个答案，以此类推。适合处理长文档，能生成更连贯、更深入的回答。- `"tree_summarize"`：将检索到的 `Node` 构建成一个树状结构，自底向上进行总结。非常适合做摘要或概述性问答。- `"no_text"`：只返回检索到的 `Node`，不调用 LLM 生成答案。- `"accumulate"`：将所有检索到的 `Node` 的内容简单拼接后返回。 | `query_engine = index.as_query_engine(response_mode="refine")` |
         | `verbose`       | `bool`            | **是否打印调试信息**。设置为 `True` 时，会打印出传递给 LLM 的完整 Prompt，非常有助于调试和优化你的 RAG 流程。 | `query_engine = index.as_query_engine(verbose=True)`         |
         | `llm`           | `BaseLLM`         | **指定用于生成答案的大语言模型**。如果你想在这个查询引擎中使用一个与全局配置不同的 LLM，可以在这里指定。 | `from llama_index.llms.openai import OpenAI``gpt4 = OpenAI(model="gpt-4")``query_engine = index.as_query_engine(llm=gpt4)` |

         3. 其他通用参数

## 二、测评体系 RAGAS

### 1.RAGAS 的核心定位与价值

RAGAS 的核心价值在于：

- **不依赖** “标准答案（Ground Truth）”（实际场景中标准答案常缺失），
- 是从 “**检索有效性**”“**生成合理性**”“**用户体验适配性**” 三个维度，拆解 RAG 系统的关键痛点，
- 输出可解释的量化分数，帮助开发者定位问题（如 “检索漏相关文档”“生成内容偏离上下文”）。

例子：金融领域 RAG 系统” 案例

用户问题：2024 年中国个人养老金账户的缴费上限是多少？

### 2.RAGAS 的核心评估指标

#### （1）Context Precision（上下文精准度）

- **定义**：衡量检索到的上下文（Context）中，“与**用户问题**真正相关的内容占比”，聚焦 “排除无关信息”。
- **计算逻辑**：**相关上下文片段数量** / 总检索上下文片段数量。

- 案例解析：

  假设系统从知识库中检索到 3 段上下文：

  - Context1：“2024 年 1 月，人社部明确个人养老金账户年缴费上限维持 12000 元，与 2023 年一致。”（**相关**）

  - Context2：“个人养老金账户可投资公募基金、储蓄存款等产品，风险等级由个人选择。”（**无关**，未提缴费上限）

  - Context3：“2022 年个人养老金账户缴费上限为 12000 元，2023 年延续该标准。”（

    **部分相关**，未明确 2024 年）

    

    若人工标注 “仅 Context1 完全相关”，则 Context Precision = 1/3 ≈ 0.33 分（分数低，说明检索混入大量无关信息，需优化检索算法，如**调整关键词权重**）。

#### （2）Context Recall（上下文召回率）

- **定义**：衡量 “知识库中所有与用户问题相关的上下文，被成功检索到的比例”，聚焦 “不遗漏关键信息”。

- **计算逻辑**：被检索到的相关上下文片段数量 / 知识库中所有相关上下文片段数量。

- 案例解析：

  假设知识库中实际有 2 段相关上下文：

  - Context1（同上，2024 年上限 12000 元）

  - Context4（“2024 年个人养老金缴费上限不调整，仍为 12000 元，政策有效期至 2025 年底”）

    若系统仅检索到 Context1，未检索到 Context4，则 Context Recall = 1/2 = 0.5 分（分数低，**说明检索漏检关键信息**，需**优化知识库索引**或**扩展检索策略，如增加同义词匹配**）。

### 2. 生成环节指标：评估 “答得准不准、好不好”

生成环节的核心是 “基于检索到的上下文，生成准确、完整、自然的回答”，RAGAS 通过 4 个指标评估生成质量：

#### （1）Faithfulness（忠实度）

- **定义**：衡量生成的回答（Answer）与检索到的上下文（Context）的 “一致性”，杜绝 “幻觉生成”（即回答中出现上下文未提及的信息）。

- **计算逻辑**：通过 LLM（如 GPT-4、Llama 3）分析回答中每个事实性陈述，判断是否能从上下文中推导得出，最终给出一致性比例。

  - 案例解析：

    检索到的 Context1 明确 “2024 年缴费上限 12000 元”，

    若系统生成回答：“2024 年中国个人养老金账户缴费上限为 15000 元，较 2023 年提高 3000 元”—— 则该回答与上下文完全矛盾，Faithfulness = 0 分（典型的幻觉生成，需**优化生成 Prompt**，增加 “**仅基于给定上下文回答**” 的约束）。

    若回答为：“根据 2024 年人社部政策，个人养老金账户年缴费上限为 12000 元”—— 则与上下文完全一致，Faithfulness = 1 分。

  #### （2）Answer Relevance（回答相关性）

  - **定义**：衡量生成的回答与用户问题的 “匹配度”，即回答是否 “紧扣问题核心，不偏离、不冗余”。

  - **计算逻辑**：通过 LLM 分析回答对 “用户问题核心需求” 的覆盖程度，排除无关内容（如用户问 “缴费上限”，回答却大谈 “投资产品”）。

  - 案例解析：

    用户问题聚焦 “2024 年缴费上限”，若系统生成回答：“个人养老金账户是国家推出的养老补充制度，2024 年可投资的公募基金超过 100 只，缴费上限方面，2023 年是 12000 元，2024 年未调整”—— 回答中 “投资基金” 属于冗余信息，核心需求（2024 年上限）表述模糊，Answer Relevance ≈ 0.4 分（需**优化生成 Prompt**，增加 **优先回答问题核心，删除无关信息**”的指令）。

#### （3）Answer Correctness（回答准确性）

- **定义**：（需依赖部分标准答案）衡量生成回答与 “客观标准答案” 的一致性，聚焦 “事实正确性”（注：RAGAS 允许标准答案不完整，仅需核心事实匹配）。

- **计算逻辑**：LLM 对比回答与标准答案的事实性差异，输出匹配比例。

- 案例解析：

  标准答案：“2024 年中国个人养老金账户年缴费上限为 12000 元”。

  若系统回答：“2024 年个人养老金账户缴费上限为 12000 元 / 年”—— 完全匹配，Correctness = 1 分；

  若回答：“2024 年个人养老金账户缴费上限为 1.2 万元”—— 单位表述不同但事实一致，Correctness = 1 分；

  若回答：“2024 年个人养老金账户缴费上限为 10000 元”—— 事实错误，Correctness = 0 分。
